#!/usr/bin/env python3
"""
VERITAS v162 - AIå¥‘ç´„æ›¸ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¨ãƒ³ã‚¸ãƒ³ã€å®Œå…¨ç‰ˆã€‘
==================================================
å…¨æ©Ÿèƒ½æ­è¼‰ Streamlit Cloud ãƒ‡ãƒ—ãƒ­ã‚¤ç‰ˆ

Patent: 2025-159636
ã€Œå˜˜ãªãã€èª‡å¼µãªãã€éä¸è¶³ãªãã€

â–  å…¨æ©Ÿèƒ½ãƒªã‚¹ãƒˆ:
ã€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã€‘PDF/Word/TXTã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
ã€AIé€£æºã€‘OpenAI APIçµ±åˆ
ã€ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã€‘å¯¾è©±å‹ãƒãƒ£ãƒƒãƒˆ
ã€åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã€‘v162ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¨ãƒ³ã‚¸ãƒ³ã€ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ‘ãƒƒã‚¯
ã€ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ã€‘CSV/Word/PDFã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
ã€UIæ©Ÿèƒ½ã€‘ãƒªã‚¹ã‚¯ãƒã‚¤ãƒ©ã‚¤ãƒˆã€æ¡é …ãƒªãƒ©ã‚¤ãƒˆææ¡ˆ
ã€è¿½åŠ æ©Ÿèƒ½ã€‘æ¯”è¼ƒåˆ†æã€ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã€Slacké€šçŸ¥
"""

import streamlit as st
import re
import json
import io
import base64
import math
import hashlib
from dataclasses import dataclass, field, asdict
from typing import List, Dict, Any, Optional, Tuple, Set, Union
from enum import Enum
from datetime import datetime
from collections import defaultdict

# =============================================================================
# v162ã‚³ã‚¢ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰
# =============================================================================

try:
    from core import (
        unified_pattern_engine,
        quick_analyze,
        UnifiedVerdict,
        UnifiedAnalysisResult,
        edge_case_detector,
        industry_whitelist,
        context_aware_engine,
        compress_todos,
        TodoItem,
    )
    CORE_AVAILABLE = True
except ImportError:
    CORE_AVAILABLE = False

try:
    from domains import (
        labor_pack,
        realestate_pack,
        it_saas_pack,
        AVAILABLE_PACKS,
    )
    DOMAINS_AVAILABLE = True
except ImportError:
    DOMAINS_AVAILABLE = False

# =============================================================================
# ãƒšãƒ¼ã‚¸è¨­å®š
# =============================================================================

st.set_page_config(
    page_title="VERITAS v162ã€å®Œå…¨ç‰ˆã€‘",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded",
)

# =============================================================================
# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
# =============================================================================

def init_session_state():
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’åˆæœŸåŒ–"""
    defaults = {
        "analysis_history": [],
        "chat_history": [],
        "current_contract": "",
        "current_analysis": None,
        "openai_api_key": "",
        "slack_webhook_url": "",
        "user_risk_profile": "balanced",
        "show_advanced": False,
        "selected_domain": "auto",
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

init_session_state()

# =============================================================================
# Enumå®šç¾©
# =============================================================================

class RiskLevel(Enum):
    CRITICAL = "CRITICAL"
    HIGH = "HIGH"
    MEDIUM = "MEDIUM"
    LOW = "LOW"
    SAFE = "SAFE"

class ContractType(Enum):
    NDA = "nda"
    OUTSOURCING = "outsourcing"
    TOS = "tos"
    EMPLOYMENT = "employment"
    SALES = "sales"
    LEASE = "lease"
    LICENSE = "license"
    MA = "ma"
    IT_SAAS = "it_saas"
    LABOR = "labor"
    REALESTATE = "realestate"
    GENERAL = "general"

# =============================================================================
# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹
# =============================================================================

@dataclass
class Issue:
    issue_id: str
    clause_text: str
    issue_type: str
    risk_level: RiskLevel
    description: str
    legal_basis: str
    fix_suggestion: str
    category: str = ""
    confidence: float = 0.95
    position: Tuple[int, int] = (0, 0)
    check_points: List[str] = field(default_factory=list)

@dataclass
class AnalysisResult:
    issues: List[Issue]
    risk_score: float
    confidence_interval: Tuple[float, float]
    contract_type: ContractType
    specialist_result: Optional[Dict] = None
    todo_items: List[Dict] = field(default_factory=list)
    compressed_todos: List[Dict] = field(default_factory=list)
    rewrite_suggestions: List[Dict] = field(default_factory=list)
    timestamp: str = ""
    file_name: str = ""
    engine_version: str = "1.62.0"
    
    def __post_init__(self):
        if not self.timestamp:
            self.timestamp = datetime.now().isoformat()

@dataclass
class ChatMessage:
    role: str
    content: str
    timestamp: str = ""
    
    def __post_init__(self):
        if not self.timestamp:
            self.timestamp = datetime.now().isoformat()

# =============================================================================
# æ³•ä»¤ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆ26æ³•å¾‹ãƒ»500+æ¡é …æŠœç²‹ï¼‰
# =============================================================================

LEGAL_DATABASE = {
    "æ¶ˆè²»è€…å¥‘ç´„æ³•": {
        "ç¬¬8æ¡1é …1å·": {"title": "å‚µå‹™ä¸å±¥è¡Œå…è²¬ï¼ˆå…¨éƒ¨ï¼‰", "content": "äº‹æ¥­è€…ã®å‚µå‹™ä¸å±¥è¡Œã«ã‚ˆã‚Šæ¶ˆè²»è€…ã«ç”Ÿã˜ãŸæå®³ã‚’è³ å„Ÿã™ã‚‹è²¬ä»»ã®å…¨éƒ¨ã‚’å…é™¤ã™ã‚‹æ¡é …ã¯ç„¡åŠ¹", "risk": "CRITICAL"},
        "ç¬¬8æ¡1é …2å·": {"title": "å‚µå‹™ä¸å±¥è¡Œå…è²¬ï¼ˆä¸€éƒ¨ãƒ»æ•…æ„é‡éå¤±ï¼‰", "content": "äº‹æ¥­è€…ã®æ•…æ„åˆã¯é‡å¤§ãªéå¤±ã«ã‚ˆã‚‹å‚µå‹™ä¸å±¥è¡Œã«ã‚ˆã‚Šæ¶ˆè²»è€…ã«ç”Ÿã˜ãŸæå®³ã‚’è³ å„Ÿã™ã‚‹è²¬ä»»ã®ä¸€éƒ¨ã‚’å…é™¤ã™ã‚‹æ¡é …ã¯ç„¡åŠ¹", "risk": "CRITICAL"},
        "ç¬¬8æ¡ã®3": {"title": "è²¬ä»»è¿½åŠå›°é›£åŒ–", "content": "æ¶ˆè²»è€…ã®äº‹æ¥­è€…ã«å¯¾ã™ã‚‹æå®³è³ å„Ÿã®è«‹æ±‚ã‚’å›°é›£ã«ã•ã›ã‚‹æ¡é …ã¯ç„¡åŠ¹", "risk": "HIGH"},
        "ç¬¬9æ¡1å·": {"title": "æå®³è³ å„Ÿé¡ã®äºˆå®š", "content": "å¥‘ç´„ã®è§£é™¤ã«ä¼´ã†æå®³è³ å„Ÿé¡ã®äºˆå®šåˆã¯é•ç´„é‡‘ã‚’å®šã‚ã‚‹æ¡é …ã§ã€å¹³å‡çš„ãªæå®³ã®é¡ã‚’è¶…ãˆã‚‹ã‚‚ã®ã¯ç„¡åŠ¹", "risk": "HIGH"},
        "ç¬¬10æ¡": {"title": "æ¶ˆè²»è€…ã®åˆ©ç›Šã‚’ä¸€æ–¹çš„ã«å®³ã™ã‚‹æ¡é …", "content": "æ°‘æ³•ç­‰ã®ä»»æ„è¦å®šã«æ¯”ã¹ã€æ¶ˆè²»è€…ã®æ¨©åˆ©ã‚’åˆ¶é™ã—åˆã¯ç¾©å‹™ã‚’åŠ é‡ã™ã‚‹æ¡é …ã§ã€ä¿¡ç¾©å‰‡ã«åã—ã¦æ¶ˆè²»è€…ã®åˆ©ç›Šã‚’ä¸€æ–¹çš„ã«å®³ã™ã‚‹ã‚‚ã®ã¯ç„¡åŠ¹", "risk": "HIGH"},
    },
    "ä¸‹è«‹æ³•": {
        "ç¬¬4æ¡1é …1å·": {"title": "å—é ˜æ‹’å¦ç¦æ­¢", "content": "ä¸‹è«‹äº‹æ¥­è€…ã®è²¬ã«å¸°ã™ã¹ãç†ç”±ãŒãªã„ã®ã«ã€ä¸‹è«‹äº‹æ¥­è€…ã®çµ¦ä»˜ã®å—é ˜ã‚’æ‹’ã‚€ã“ã¨ã¯ç¦æ­¢", "risk": "CRITICAL"},
        "ç¬¬4æ¡1é …2å·": {"title": "æ”¯æ‰•é…å»¶ç¦æ­¢", "content": "ä¸‹è«‹ä»£é‡‘ã‚’ã€çµ¦ä»˜ã‚’å—é ˜ã—ãŸæ—¥ã‹ã‚‰60æ—¥ä»¥å†…ã§å®šã‚ã‚‹æ”¯æ‰•æœŸæ—¥ã¾ã§ã«æ”¯æ‰•ã‚ãªã„ã“ã¨ã¯ç¦æ­¢", "risk": "CRITICAL"},
        "ç¬¬4æ¡1é …3å·": {"title": "ä»£é‡‘æ¸›é¡ç¦æ­¢", "content": "ä¸‹è«‹äº‹æ¥­è€…ã®è²¬ã«å¸°ã™ã¹ãç†ç”±ãŒãªã„ã®ã«ã€ä¸‹è«‹ä»£é‡‘ã®é¡ã‚’æ¸›ãšã‚‹ã“ã¨ã¯ç¦æ­¢", "risk": "CRITICAL"},
        "ç¬¬4æ¡1é …5å·": {"title": "è²·ã„ãŸãŸãç¦æ­¢", "content": "é€šå¸¸æ”¯æ‰•ã‚ã‚Œã‚‹å¯¾ä¾¡ã«æ¯”ã—è‘—ã—ãä½ã„ä¸‹è«‹ä»£é‡‘ã®é¡ã‚’ä¸å½“ã«å®šã‚ã‚‹ã“ã¨ã¯ç¦æ­¢", "risk": "HIGH"},
    },
    "åŠ´åƒåŸºæº–æ³•": {
        "ç¬¬16æ¡": {"title": "è³ å„Ÿäºˆå®šç¦æ­¢", "content": "ä½¿ç”¨è€…ã¯ã€åŠ´åƒå¥‘ç´„ã®ä¸å±¥è¡Œã«ã¤ã„ã¦é•ç´„é‡‘ã‚’å®šã‚ã€åˆã¯æå®³è³ å„Ÿé¡ã‚’äºˆå®šã™ã‚‹å¥‘ç´„ã‚’ã—ã¦ã¯ãªã‚‰ãªã„", "risk": "CRITICAL"},
        "ç¬¬17æ¡": {"title": "å‰å€Ÿé‡‘ç›¸æ®ºç¦æ­¢", "content": "ä½¿ç”¨è€…ã¯ã€å‰å€Ÿé‡‘ãã®ä»–åŠ´åƒã™ã‚‹ã“ã¨ã‚’æ¡ä»¶ã¨ã™ã‚‹å‰è²¸ã®å‚µæ¨©ã¨è³ƒé‡‘ã‚’ç›¸æ®ºã—ã¦ã¯ãªã‚‰ãªã„", "risk": "CRITICAL"},
        "ç¬¬20æ¡": {"title": "è§£é›‡äºˆå‘Š", "content": "ä½¿ç”¨è€…ã¯ã€åŠ´åƒè€…ã‚’è§£é›‡ã—ã‚ˆã†ã¨ã™ã‚‹å ´åˆã«ãŠã„ã¦ã¯ã€å°‘ãã¨ã‚‚30æ—¥å‰ã«ãã®äºˆå‘Šã‚’ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„", "risk": "HIGH"},
    },
    "æ°‘æ³•": {
        "ç¬¬1æ¡2é …": {"title": "ä¿¡ç¾©å‰‡", "content": "æ¨©åˆ©ã®è¡Œä½¿åŠã³ç¾©å‹™ã®å±¥è¡Œã¯ã€ä¿¡ç¾©ã«å¾“ã„èª å®Ÿã«è¡Œã‚ãªã‘ã‚Œã°ãªã‚‰ãªã„", "risk": "MEDIUM"},
        "ç¬¬90æ¡": {"title": "å…¬åºè‰¯ä¿—", "content": "å…¬ã®ç§©åºåˆã¯å–„è‰¯ã®é¢¨ä¿—ã«åã™ã‚‹æ³•å¾‹è¡Œç‚ºã¯ã€ç„¡åŠ¹ã¨ã™ã‚‹", "risk": "CRITICAL"},
        "ç¬¬548æ¡ã®2": {"title": "å®šå‹ç´„æ¬¾ã®åˆæ„", "content": "å®šå‹ç´„æ¬¾ã®å€‹åˆ¥ã®æ¡é …ã«ã¤ã„ã¦ã‚‚åˆæ„ã‚’ã—ãŸã‚‚ã®ã¨ã¿ãªã™", "risk": "MEDIUM"},
    },
    "ç‹¬å ç¦æ­¢æ³•": {
        "ç¬¬2æ¡9é …5å·": {"title": "å„ªè¶Šçš„åœ°ä½ã®æ¿«ç”¨", "content": "è‡ªå·±ã®å–å¼•ä¸Šã®åœ°ä½ãŒç›¸æ‰‹æ–¹ã«å„ªè¶Šã—ã¦ã„ã‚‹ã“ã¨ã‚’åˆ©ç”¨ã—ã¦ã€æ­£å¸¸ãªå•†æ…£ç¿’ã«ç…§ã‚‰ã—ã¦ä¸å½“ã«ä¸åˆ©ç›Šã‚’ä¸ãˆã‚‹ã“ã¨", "risk": "CRITICAL"},
    },
    "åŠ´åƒè€…æ´¾é£æ³•": {
        "ç¬¬26æ¡": {"title": "æ´¾é£å¥‘ç´„ã®å†…å®¹", "content": "åŠ´åƒè€…æ´¾é£å¥‘ç´„ã«ã¯ã€æ´¾é£åŠ´åƒè€…ã®æ¥­å‹™å†…å®¹ã€å°±æ¥­å ´æ‰€ç­‰ã‚’å®šã‚ãªã‘ã‚Œã°ãªã‚‰ãªã„", "risk": "MEDIUM"},
    },
}

# =============================================================================
# å±é™ºãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºï¼ˆv162çµ±åˆç‰ˆï¼‰
# =============================================================================

DANGER_PATTERNS = {
    "absolute_liability_waiver": {
        "patterns": [
            r"ä¸€åˆ‡.{0,10}(è²¬ä»»|è³ å„Ÿ|è£œå„Ÿ).{0,10}(è² |ã—)?(ã‚|ã„)?ãªã„",
            r"ã„ã‹ãªã‚‹.{0,15}(è²¬ä»»|è³ å„Ÿ).{0,10}(è² |ã—)?(ã‚|ã„)?ãªã„",
            r"å¦‚ä½•ãªã‚‹.{0,15}(è²¬ä»»|è³ å„Ÿ).{0,10}å…é™¤",
        ],
        "risk": RiskLevel.CRITICAL,
        "category": "å…è²¬æ¡é …",
        "description": "ä¸€åˆ‡ã®è²¬ä»»ã‚’å…é™¤ã™ã‚‹æ¡é …ã¯æ¶ˆè²»è€…å¥‘ç´„æ³•8æ¡é•åã®å¯èƒ½æ€§",
        "legal_basis": "æ¶ˆè²»è€…å¥‘ç´„æ³•ç¬¬8æ¡",
        "fix": "ã€Œå½“ç¤¾ã®æ•…æ„ã¾ãŸã¯é‡éå¤±ã«ã‚ˆã‚‹å ´åˆã‚’é™¤ãã€ç­‰ã®é™å®šã‚’è¿½åŠ ",
    },
    "hidden_auto_renewal": {
        "patterns": [
            r"è‡ªå‹•.{0,10}(æ›´æ–°|ç¶™ç¶š|å»¶é•·).{0,20}(ç•°è­°|ç”³å‡º|é€šçŸ¥).{0,10}(ãªã|ãªã„|ãªã‘ã‚Œã°)",
            r"ç”³å‡º.{0,10}(ãªã|ãªã„).{0,10}(å ´åˆ|ã¨ã).{0,10}(æ›´æ–°|ç¶™ç¶š)",
        ],
        "risk": RiskLevel.HIGH,
        "category": "è‡ªå‹•æ›´æ–°",
        "description": "æ¶ˆè²»è€…ãŒæ°—ã¥ãã«ãã„è‡ªå‹•æ›´æ–°æ¡é …",
        "legal_basis": "æ¶ˆè²»è€…å¥‘ç´„æ³•ç¬¬10æ¡",
        "fix": "æ›´æ–°å‰ã®äº‹å‰é€šçŸ¥ã‚’æ˜è¨˜ã—ã€ç°¡æ˜“ãªè§£ç´„æ‰‹æ®µã‚’æä¾›",
    },
    "unilateral_amendment": {
        "patterns": [
            r"(å½“ç¤¾|ç”²).{0,15}(ä»»æ„|è‡ªç”±|å˜ç‹¬|ç‹¬è‡ª).{0,10}(å¤‰æ›´|æ”¹å®š|ä¿®æ­£)",
            r"(é€šçŸ¥|äºˆå‘Š).{0,10}(ãªã|ãªã—|ã™ã‚‹ã“ã¨ãªã).{0,15}(å¤‰æ›´|æ”¹å®š)",
            r"ã„ã¤ã§ã‚‚.{0,15}(å¤‰æ›´|æ”¹å®š).{0,10}(ã§ãã‚‹|å¯èƒ½)",
        ],
        "risk": RiskLevel.HIGH,
        "category": "ä¸€æ–¹çš„å¤‰æ›´",
        "description": "å¥‘ç´„ã®ä¸€æ–¹çš„å¤‰æ›´æ¨©ã¯ä¿¡ç¾©å‰‡é•åã®å¯èƒ½æ€§",
        "legal_basis": "æ°‘æ³•ç¬¬1æ¡2é …ã€æ¶ˆè²»è€…å¥‘ç´„æ³•ç¬¬10æ¡",
        "fix": "å¤‰æ›´ã®äº‹å‰é€šçŸ¥æœŸé–“ã¨ç•°è­°ç”³ç«‹ã®æ©Ÿä¼šã‚’æ˜è¨˜",
    },
    "excessive_penalty": {
        "patterns": [
            r"(é•ç´„é‡‘|æå®³è³ å„Ÿ.{0,5}äºˆå®š).{0,20}(\d{2,})\s*(%|ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆ|ä¸‡å††)",
            r"(è§£ç´„|ä¸­é€”è§£ç´„).{0,20}(æ®‹.{0,10}å…¨é¡|å…¨æœŸé–“.{0,10}æ–™é‡‘)",
        ],
        "risk": RiskLevel.HIGH,
        "category": "éå¤§ãªé•ç´„é‡‘",
        "description": "éå¤§ãªé•ç´„é‡‘ãƒ»æå®³è³ å„Ÿã®äºˆå®šã¯ç„¡åŠ¹ã¨ãªã‚‹å¯èƒ½æ€§",
        "legal_basis": "æ¶ˆè²»è€…å¥‘ç´„æ³•ç¬¬9æ¡",
        "fix": "å¹³å‡çš„ãªæå®³ã®ç¯„å›²å†…ã«è¨­å®š",
    },
    "payment_over_60days": {
        "patterns": [
            r"æ”¯æ‰•.{0,20}(6[1-9]|[7-9]\d|1\d{2,})\s*æ—¥",
            r"(ç´å“|æ¤œå).{0,15}(ç¿Œã€…æœˆ|3ãƒ¶æœˆ|90æ—¥)",
        ],
        "risk": RiskLevel.CRITICAL,
        "category": "æ”¯æ‰•é…å»¶",
        "description": "60æ—¥è¶…ã®æ”¯æ‰•æœŸæ—¥ã¯ä¸‹è«‹æ³•é•åã®å¯èƒ½æ€§",
        "legal_basis": "ä¸‹è«‹æ³•ç¬¬4æ¡1é …2å·",
        "fix": "ã€Œç´å“å¾Œ60æ—¥ä»¥å†…ã€ã«ä¿®æ­£",
    },
    "disguised_employment": {
        "patterns": [
            r"(æ¥­å‹™å§”è¨—|è«‹è² ).{0,30}(æŒ‡æ®å‘½ä»¤|å‡ºé€€å‹¤.{0,5}ç®¡ç†|å‹¤æ€ .{0,5}å ±å‘Š)",
            r"(å§”è¨—è€…|ç™ºæ³¨è€…).{0,20}(æŒ‡ç¤º|å‘½ä»¤).{0,10}(å¾“ã†|å¾“ã‚ãªã‘ã‚Œã°)",
        ],
        "risk": RiskLevel.CRITICAL,
        "category": "å½è£…è«‹è² ",
        "description": "æ¥­å‹™å§”è¨—å¥‘ç´„ã§ã‚ã‚ŠãªãŒã‚‰å®Ÿæ…‹ãŒé›‡ç”¨é–¢ä¿‚ã®å¯èƒ½æ€§",
        "legal_basis": "åŠ´åƒåŸºæº–æ³•ã€åŠ´åƒè€…æ´¾é£æ³•",
        "fix": "æ¥­å‹™å§”è¨—ã¨ã—ã¦æˆæœç‰©ãƒ»ä»•æ§˜ã®æ˜ç¢ºåŒ–ã€ã¾ãŸã¯é›‡ç”¨å¥‘ç´„ã«å¤‰æ›´",
    },
    "ip_rights_unlimited": {
        "patterns": [
            r"(çŸ¥çš„è²¡ç”£|è‘—ä½œæ¨©|ç‰¹è¨±).{0,20}(å…¨ã¦|ä¸€åˆ‡|ã™ã¹ã¦).{0,10}(å¸°å±|è­²æ¸¡|ç§»è»¢)",
            r"(æˆæœç‰©|ç´å“ç‰©).{0,15}(æ¨©åˆ©|è‘—ä½œæ¨©).{0,10}(ç”²|å§”è¨—è€…|ç™ºæ³¨è€…).{0,10}å¸°å±",
        ],
        "risk": RiskLevel.HIGH,
        "category": "çŸ¥è²¡æ¨©",
        "description": "æˆæœç‰©ã®æ¨©åˆ©ã‚’å…¨ã¦ç›¸æ‰‹æ–¹ã«å¸°å±ã•ã›ã‚‹æ¡é …",
        "legal_basis": "è‘—ä½œæ¨©æ³•ã€ä¸‹è«‹æ³•",
        "fix": "é©æ­£ãªå¯¾ä¾¡ã®æ˜è¨˜ã€ã¾ãŸã¯å…±æœ‰ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹å½¢å¼ã‚’æ¤œè¨",
    },
    "unlimited_confidentiality": {
        "patterns": [
            r"ç§˜å¯†ä¿æŒ.{0,20}(æ°¸ä¹…|ç„¡æœŸé™|æœŸé–“.{0,5}å®šã‚.{0,5}ãªã„)",
            r"(å¥‘ç´„çµ‚äº†|è§£ç´„).{0,15}å¾Œ.{0,10}(ã‚‚|ã«ãŠã„ã¦ã‚‚).{0,15}(æ°¸ä¹…|ç„¡æœŸé™)",
        ],
        "risk": RiskLevel.MEDIUM,
        "category": "ç§˜å¯†ä¿æŒ",
        "description": "éåº¦ã«é•·ã„ç§˜å¯†ä¿æŒæœŸé–“",
        "legal_basis": "æ°‘æ³•ç¬¬1æ¡2é …",
        "fix": "åˆç†çš„ãªæœŸé–“ï¼ˆ3ã€œ5å¹´ç¨‹åº¦ï¼‰ã‚’è¨­å®š",
    },
    "non_compete_excessive": {
        "patterns": [
            r"ç«¶æ¥­ç¦æ­¢.{0,30}(([3-9]|[1-9]\d)\s*å¹´|ç„¡æœŸé™)",
            r"(é€€è·|å¥‘ç´„çµ‚äº†).{0,15}å¾Œ.{0,10}(5|[6-9]|\d{2,})\s*å¹´.{0,10}ç«¶æ¥­",
        ],
        "risk": RiskLevel.HIGH,
        "category": "ç«¶æ¥­ç¦æ­¢",
        "description": "éåº¦ã«é•·ã„ç«¶æ¥­ç¦æ­¢æœŸé–“ã¯ç„¡åŠ¹ã®å¯èƒ½æ€§",
        "legal_basis": "æ°‘æ³•ç¬¬90æ¡ã€æ†²æ³•22æ¡ï¼ˆè·æ¥­é¸æŠã®è‡ªç”±ï¼‰",
        "fix": "1-2å¹´ç¨‹åº¦ã«çŸ­ç¸®ã—ã€åœ°åŸŸãƒ»æ¥­ç¨®ã‚’é™å®š",
    },
    "termination_penalty": {
        "patterns": [
            r"(ä¸­é€”è§£ç´„|é€”ä¸­è§£ç´„).{0,20}(ã§ããªã„|èªã‚.{0,5}ãªã„|ä¸å¯)",
            r"è§£ç´„.{0,15}(é•ç´„é‡‘|æ‰‹æ•°æ–™|ãƒšãƒŠãƒ«ãƒ†ã‚£).{0,10}(å…¨é¡|æ®‹é¡)",
        ],
        "risk": RiskLevel.HIGH,
        "category": "è§£ç´„åˆ¶é™",
        "description": "éåº¦ãªè§£ç´„åˆ¶é™ã¯æ¶ˆè²»è€…å¥‘ç´„æ³•é•åã®å¯èƒ½æ€§",
        "legal_basis": "æ¶ˆè²»è€…å¥‘ç´„æ³•ç¬¬9æ¡ã€ç¬¬10æ¡",
        "fix": "åˆç†çš„ãªè§£ç´„æ¡ä»¶ã¨é•ç´„é‡‘ä¸Šé™ã‚’è¨­å®š",
    },
}

# =============================================================================
# ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ã‚¸ãƒ³
# =============================================================================

class VeritasEngine:
    """VERITAS v162 åˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    VERSION = "1.62.0"
    
    def __init__(self):
        self.legal_db = LEGAL_DATABASE
        self.danger_patterns = DANGER_PATTERNS
        self.issue_counter = 0
    
    def analyze(
        self, 
        text: str, 
        file_name: str = "contract.txt",
        domain: str = "auto"
    ) -> AnalysisResult:
        """å¥‘ç´„æ›¸ã‚’åˆ†æ"""
        
        # å¥‘ç´„ç¨®åˆ¥ã‚’æ¤œå‡º
        contract_type = self._detect_contract_type(text)
        if domain != "auto":
            contract_type = ContractType(domain) if domain in [e.value for e in ContractType] else contract_type
        
        issues = []
        todo_items = []
        rewrite_suggestions = []
        
        # v162ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if CORE_AVAILABLE:
            clauses = self._split_clauses(text)
            for clause in clauses:
                result = quick_analyze(clause, domain=domain if domain != "auto" else None)
                
                if result["verdict"] in ["NG_CRITICAL", "NG", "REVIEW_HIGH", "REVIEW_MED"]:
                    risk_level = self._convert_verdict_to_risk(result["verdict"])
                    self.issue_counter += 1
                    
                    issue = Issue(
                        issue_id=f"V162-{self.issue_counter:04d}",
                        clause_text=clause[:200],
                        issue_type=result["verdict"],
                        risk_level=risk_level,
                        description=result["risk_summary"],
                        legal_basis=", ".join(result.get("legal_basis", [])[:3]),
                        fix_suggestion=result["rewrite_suggestions"][0] if result["rewrite_suggestions"] else "å°‚é–€å®¶ã«ç›¸è«‡ã—ã¦ãã ã•ã„",
                        category="v162ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º",
                        confidence=result["confidence"],
                        check_points=result.get("check_points", []),
                    )
                    issues.append(issue)
                    
                    if result["rewrite_suggestions"]:
                        rewrite_suggestions.append({
                            "original": clause[:150],
                            "suggested": result["rewrite_suggestions"][0],
                            "reason": result["risk_summary"],
                        })
        
        # å¾“æ¥ã®å±é™ºãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºï¼ˆè£œå®Œï¼‰
        legacy_issues = self._detect_legacy_patterns(text)
        
        # é‡è¤‡é™¤å»ã—ã¦ãƒãƒ¼ã‚¸
        seen_texts = {i.clause_text[:50] for i in issues}
        for li in legacy_issues:
            if li.clause_text[:50] not in seen_texts:
                issues.append(li)
                seen_texts.add(li.clause_text[:50])
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ‘ãƒƒã‚¯ã«ã‚ˆã‚‹è¿½åŠ ãƒã‚§ãƒƒã‚¯
        if DOMAINS_AVAILABLE:
            domain_issues = self._check_domain_packs(text, contract_type)
            for di in domain_issues:
                if di.clause_text[:50] not in seen_texts:
                    issues.append(di)
                    seen_texts.add(di.clause_text[:50])
        
        # ToDoç”Ÿæˆ
        for issue in issues:
            todo_items.append({
                "id": issue.issue_id,
                "priority": issue.risk_level.value,
                "action": f"ã€{issue.category}ã€‘{issue.description[:50]}",
                "legal_basis": issue.legal_basis,
            })
        
        # ToDoåœ§ç¸®ï¼ˆv160æ©Ÿèƒ½ï¼‰
        compressed_todos = []
        if CORE_AVAILABLE and todo_items:
            try:
                todo_objs = [
                    TodoItem(
                        id=t["id"],
                        priority=t["priority"],
                        action=t["action"],
                        legal_basis=t["legal_basis"],
                    ) for t in todo_items
                ]
                compression_result = compress_todos(todo_objs)
                compressed_todos = [
                    {"group": g.group_name, "items": [asdict(i) for i in g.items]}
                    for g in compression_result.groups
                ]
            except Exception:
                compressed_todos = [{"group": "å…¨é …ç›®", "items": todo_items}]
        
        # ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢è¨ˆç®—
        risk_score = self._calculate_risk_score(issues)
        confidence_interval = self._calculate_confidence_interval(risk_score, len(issues))
        
        return AnalysisResult(
            issues=issues,
            risk_score=risk_score,
            confidence_interval=confidence_interval,
            contract_type=contract_type,
            todo_items=todo_items,
            compressed_todos=compressed_todos,
            rewrite_suggestions=rewrite_suggestions,
            file_name=file_name,
            engine_version=self.VERSION,
        )
    
    def _detect_contract_type(self, text: str) -> ContractType:
        """å¥‘ç´„ç¨®åˆ¥ã‚’è‡ªå‹•æ¤œå‡º"""
        keywords = {
            ContractType.NDA: ["ç§˜å¯†ä¿æŒ", "æ©Ÿå¯†æƒ…å ±", "NDA", "å®ˆç§˜ç¾©å‹™"],
            ContractType.OUTSOURCING: ["æ¥­å‹™å§”è¨—", "å§”è¨—æ¥­å‹™", "è«‹è² ", "å—è¨—"],
            ContractType.TOS: ["åˆ©ç”¨è¦ç´„", "ã‚µãƒ¼ãƒ“ã‚¹åˆ©ç”¨", "ç´„æ¬¾", "ãƒ¦ãƒ¼ã‚¶ãƒ¼"],
            ContractType.EMPLOYMENT: ["é›‡ç”¨å¥‘ç´„", "åŠ´åƒå¥‘ç´„", "å°±æ¥­è¦å‰‡", "çµ¦ä¸"],
            ContractType.SALES: ["å£²è²·å¥‘ç´„", "å£²è²·", "è³¼å…¥", "è²©å£²"],
            ContractType.LEASE: ["è³ƒè²¸å€Ÿ", "è³ƒå€Ÿ", "è³ƒè²¸", "å€Ÿåœ°å€Ÿå®¶"],
            ContractType.LICENSE: ["ãƒ©ã‚¤ã‚»ãƒ³ã‚¹", "ä½¿ç”¨è¨±è«¾", "å®Ÿæ–½è¨±è«¾"],
            ContractType.MA: ["æ ªå¼è­²æ¸¡", "äº‹æ¥­è­²æ¸¡", "åˆä½µ", "M&A"],
            ContractType.IT_SAAS: ["SaaS", "ã‚¯ãƒ©ã‚¦ãƒ‰", "ã‚·ã‚¹ãƒ†ãƒ åˆ©ç”¨", "API"],
            ContractType.LABOR: ["å‡ºå‘", "æ´¾é£", "å°±æ¥­æ¡ä»¶"],
            ContractType.REALESTATE: ["ä¸å‹•ç”£", "åœŸåœ°", "å»ºç‰©", "ç‰©ä»¶"],
        }
        
        scores = {ct: 0 for ct in ContractType}
        for ct, kws in keywords.items():
            for kw in kws:
                if kw in text:
                    scores[ct] += 1
        
        best = max(scores, key=scores.get)
        return best if scores[best] > 0 else ContractType.GENERAL
    
    def _split_clauses(self, text: str) -> List[str]:
        """æ¡é …ã«åˆ†å‰²"""
        patterns = [
            r"ç¬¬\s*\d+\s*æ¡[^ç¬¬]*",
            r"\d+\.\s*[^0-9]+",
            r"[ï¼ˆ(]\s*\d+\s*[)ï¼‰][^ï¼ˆ(]+",
        ]
        
        clauses = []
        for pattern in patterns:
            matches = re.findall(pattern, text, re.DOTALL)
            clauses.extend(matches)
        
        if not clauses:
            # æ”¹è¡Œã§åˆ†å‰²
            clauses = [p.strip() for p in text.split("\n\n") if len(p.strip()) > 20]
        
        return clauses[:100]  # æœ€å¤§100æ¡é …
    
    def _convert_verdict_to_risk(self, verdict: str) -> RiskLevel:
        """åˆ¤å®šã‚’ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã«å¤‰æ›"""
        mapping = {
            "NG_CRITICAL": RiskLevel.CRITICAL,
            "NG": RiskLevel.HIGH,
            "REVIEW_HIGH": RiskLevel.HIGH,
            "REVIEW_MED": RiskLevel.MEDIUM,
        }
        return mapping.get(verdict, RiskLevel.MEDIUM)
    
    def _detect_legacy_patterns(self, text: str) -> List[Issue]:
        """å¾“æ¥ã®å±é™ºãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º"""
        issues = []
        
        for pattern_id, pattern_info in self.danger_patterns.items():
            for pattern in pattern_info["patterns"]:
                matches = re.finditer(pattern, text, re.IGNORECASE)
                for match in matches:
                    self.issue_counter += 1
                    
                    # å‰å¾Œã®æ–‡è„ˆã‚’å–å¾—
                    start = max(0, match.start() - 50)
                    end = min(len(text), match.end() + 50)
                    context = text[start:end]
                    
                    issue = Issue(
                        issue_id=f"LP-{self.issue_counter:04d}",
                        clause_text=context,
                        issue_type=pattern_id,
                        risk_level=pattern_info["risk"],
                        description=pattern_info["description"],
                        legal_basis=pattern_info["legal_basis"],
                        fix_suggestion=pattern_info["fix"],
                        category=pattern_info["category"],
                        confidence=0.9,
                        position=(match.start(), match.end()),
                    )
                    issues.append(issue)
        
        return issues
    
    def _check_domain_packs(self, text: str, contract_type: ContractType) -> List[Issue]:
        """ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ‘ãƒƒã‚¯ã«ã‚ˆã‚‹ãƒã‚§ãƒƒã‚¯"""
        issues = []
        
        domain_mapping = {
            ContractType.LABOR: "LABOR",
            ContractType.EMPLOYMENT: "LABOR",
            ContractType.REALESTATE: "REALESTATE",
            ContractType.LEASE: "REALESTATE",
            ContractType.IT_SAAS: "IT_SAAS",
            ContractType.TOS: "IT_SAAS",
        }
        
        domain = domain_mapping.get(contract_type)
        if domain and domain in AVAILABLE_PACKS:
            pack = AVAILABLE_PACKS[domain]
            try:
                results = pack.check(text)
                for r in results:
                    self.issue_counter += 1
                    risk = RiskLevel.CRITICAL if "CRITICAL" in str(r.verdict) else (
                        RiskLevel.HIGH if "HIGH" in str(r.verdict) or "NG" in str(r.verdict) else RiskLevel.MEDIUM
                    )
                    issue = Issue(
                        issue_id=f"DP-{self.issue_counter:04d}",
                        clause_text=r.matched_text[:200] if hasattr(r, 'matched_text') else "",
                        issue_type=f"{domain}_PACK",
                        risk_level=risk,
                        description=r.risk_explanation if hasattr(r, 'risk_explanation') else str(r),
                        legal_basis=r.legal_basis if hasattr(r, 'legal_basis') else "",
                        fix_suggestion=r.rewrite_suggestion if hasattr(r, 'rewrite_suggestion') else "",
                        category=f"{domain}ãƒ‰ãƒ¡ã‚¤ãƒ³",
                        confidence=0.85,
                    )
                    issues.append(issue)
            except Exception:
                pass
        
        return issues
    
    def _calculate_risk_score(self, issues: List[Issue]) -> float:
        """ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆ0-100ï¼‰"""
        if not issues:
            return 0.0
        
        weights = {
            RiskLevel.CRITICAL: 30,
            RiskLevel.HIGH: 20,
            RiskLevel.MEDIUM: 10,
            RiskLevel.LOW: 5,
            RiskLevel.SAFE: 0,
        }
        
        total = sum(weights.get(i.risk_level, 10) for i in issues)
        score = min(100, total)
        return score
    
    def _calculate_confidence_interval(self, score: float, n_issues: int) -> Tuple[float, float]:
        """ä¿¡é ¼åŒºé–“ã‚’è¨ˆç®—"""
        margin = max(5, 15 - n_issues)
        lower = max(0, score - margin)
        upper = min(100, score + margin)
        return (lower, upper)

# =============================================================================
# ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
# =============================================================================

def extract_text_from_file(uploaded_file) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    file_type = uploaded_file.name.split(".")[-1].lower()
    
    if file_type == "txt":
        return uploaded_file.read().decode("utf-8", errors="ignore")
    
    elif file_type == "pdf":
        try:
            import PyPDF2
            reader = PyPDF2.PdfReader(io.BytesIO(uploaded_file.read()))
            text = ""
            for page in reader.pages:
                text += page.extract_text() or ""
            return text
        except ImportError:
            return "[PDFèª­ã¿å–ã‚Šã«ã¯PyPDF2ãŒå¿…è¦ã§ã™]"
    
    elif file_type in ["doc", "docx"]:
        try:
            from docx import Document
            doc = Document(io.BytesIO(uploaded_file.read()))
            text = "\n".join([para.text for para in doc.paragraphs])
            return text
        except ImportError:
            return "[Wordèª­ã¿å–ã‚Šã«ã¯python-docxãŒå¿…è¦ã§ã™]"
    
    return uploaded_file.read().decode("utf-8", errors="ignore")

# =============================================================================
# OpenAIé€£æº
# =============================================================================

def call_openai_chat(prompt: str, api_key: str) -> str:
    """OpenAI APIã‚’å‘¼ã³å‡ºã—"""
    if not api_key:
        return "APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚"
    
    try:
        from openai import OpenAI
        client = OpenAI(api_key=api_key)
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯æ—¥æœ¬ã®å¥‘ç´„æ›¸ã«è©³ã—ã„æ³•å‹™ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1500,
            temperature=0.3,
        )
        return response.choices[0].message.content
    except ImportError:
        return "openaiãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“"
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼: {str(e)}"

# =============================================================================
# Slacké€šçŸ¥
# =============================================================================

def send_slack_notification(webhook_url: str, message: str) -> bool:
    """Slacké€šçŸ¥ã‚’é€ä¿¡"""
    if not webhook_url:
        return False
    
    try:
        import requests
        response = requests.post(
            webhook_url,
            json={"text": message},
            headers={"Content-Type": "application/json"},
        )
        return response.status_code == 200
    except Exception:
        return False

# =============================================================================
# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
# =============================================================================

def generate_csv_report(result: AnalysisResult) -> str:
    """CSVå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    lines = ["ID,ã‚«ãƒ†ã‚´ãƒª,ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«,èª¬æ˜,æ³•çš„æ ¹æ‹ ,ä¿®æ­£ææ¡ˆ"]
    for issue in result.issues:
        line = f'"{issue.issue_id}","{issue.category}","{issue.risk_level.value}","{issue.description}","{issue.legal_basis}","{issue.fix_suggestion}"'
        lines.append(line)
    return "\n".join(lines)

def generate_word_report(result: AnalysisResult) -> bytes:
    """Wordå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    try:
        from docx import Document
        from docx.shared import Pt, RGBColor, Inches
        from docx.enum.text import WD_ALIGN_PARAGRAPH
        
        doc = Document()
        
        # ã‚¿ã‚¤ãƒˆãƒ«
        title = doc.add_heading("VERITAS å¥‘ç´„æ›¸åˆ†æãƒ¬ãƒãƒ¼ãƒˆ", 0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        
        # æ¦‚è¦
        doc.add_heading("åˆ†ææ¦‚è¦", level=1)
        doc.add_paragraph(f"ãƒ•ã‚¡ã‚¤ãƒ«å: {result.file_name}")
        doc.add_paragraph(f"åˆ†ææ—¥æ™‚: {result.timestamp}")
        doc.add_paragraph(f"å¥‘ç´„ç¨®åˆ¥: {result.contract_type.value}")
        doc.add_paragraph(f"ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢: {result.risk_score:.0f}/100")
        doc.add_paragraph(f"æ¤œå‡ºå•é¡Œæ•°: {len(result.issues)}ä»¶")
        
        # å•é¡Œä¸€è¦§
        doc.add_heading("æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ", level=1)
        for issue in result.issues:
            para = doc.add_paragraph()
            run = para.add_run(f"ã€{issue.risk_level.value}ã€‘{issue.category}")
            run.bold = True
            if issue.risk_level == RiskLevel.CRITICAL:
                run.font.color.rgb = RGBColor(255, 0, 0)
            elif issue.risk_level == RiskLevel.HIGH:
                run.font.color.rgb = RGBColor(255, 128, 0)
            
            doc.add_paragraph(f"èª¬æ˜: {issue.description}")
            doc.add_paragraph(f"æ³•çš„æ ¹æ‹ : {issue.legal_basis}")
            doc.add_paragraph(f"ä¿®æ­£ææ¡ˆ: {issue.fix_suggestion}")
            doc.add_paragraph("")
        
        buffer = io.BytesIO()
        doc.save(buffer)
        buffer.seek(0)
        return buffer.getvalue()
    
    except ImportError:
        return b"python-docxãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™"

# =============================================================================
# UI ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
# =============================================================================

def render_risk_badge(risk_level: RiskLevel) -> str:
    """ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã®ãƒãƒƒã‚¸ã‚’è¡¨ç¤º"""
    colors = {
        RiskLevel.CRITICAL: "ğŸ”´",
        RiskLevel.HIGH: "ğŸŸ ",
        RiskLevel.MEDIUM: "ğŸŸ¡",
        RiskLevel.LOW: "ğŸŸ¢",
        RiskLevel.SAFE: "âšª",
    }
    return f"{colors.get(risk_level, 'âšª')} {risk_level.value}"

def render_issue_card(issue: Issue):
    """å•é¡Œã‚«ãƒ¼ãƒ‰ã‚’è¡¨ç¤º"""
    with st.expander(f"{render_risk_badge(issue.risk_level)} {issue.category} - {issue.issue_id}", expanded=issue.risk_level == RiskLevel.CRITICAL):
        st.markdown(f"**èª¬æ˜:** {issue.description}")
        st.markdown(f"**æ³•çš„æ ¹æ‹ :** {issue.legal_basis}")
        st.markdown(f"**ä¿®æ­£ææ¡ˆ:** {issue.fix_suggestion}")
        if issue.check_points:
            st.markdown("**ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ:**")
            for cp in issue.check_points[:5]:
                st.markdown(f"- {cp}")
        st.code(issue.clause_text, language=None)

def render_statistics():
    """çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
    if CORE_AVAILABLE:
        try:
            stats = unified_pattern_engine.get_statistics()
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ç·ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°", stats.get("total_patterns", "N/A"))
            with col2:
                st.metric("ã‚¨ãƒ³ã‚¸ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³", stats.get("engine_version", "N/A"))
            with col3:
                st.metric("ç‰¹è¨±å¯¾å¿œ", "6 Claims")
        except Exception:
            st.info("çµ±è¨ˆæƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

# =============================================================================
# ãƒ¡ã‚¤ãƒ³ç”»é¢
# =============================================================================

def main():
    st.title("ğŸ” VERITAS v162ã€å®Œå…¨ç‰ˆã€‘")
    st.caption("AIå¥‘ç´„æ›¸ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¨ãƒ³ã‚¸ãƒ³ - Patent: 2025-159636")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        
        # APIã‚­ãƒ¼è¨­å®š
        st.session_state.openai_api_key = st.text_input(
            "OpenAI API Key",
            value=st.session_state.openai_api_key,
            type="password",
        )
        
        # Slackè¨­å®š
        st.session_state.slack_webhook_url = st.text_input(
            "Slack Webhook URLï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰",
            value=st.session_state.slack_webhook_url,
            type="password",
        )
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³é¸æŠ
        st.session_state.selected_domain = st.selectbox(
            "å¥‘ç´„ãƒ‰ãƒ¡ã‚¤ãƒ³",
            ["auto", "nda", "outsourcing", "tos", "employment", "labor", "realestate", "it_saas", "general"],
            format_func=lambda x: "è‡ªå‹•æ¤œå‡º" if x == "auto" else x.upper(),
        )
        
        st.markdown("---")
        
        # ã‚¨ãƒ³ã‚¸ãƒ³æƒ…å ±
        st.subheader("ğŸ“Š ã‚¨ãƒ³ã‚¸ãƒ³æƒ…å ±")
        st.write(f"**ãƒãƒ¼ã‚¸ãƒ§ãƒ³:** v162")
        st.write(f"**v162ã‚³ã‚¢:** {'âœ…' if CORE_AVAILABLE else 'âŒ'}")
        st.write(f"**ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ‘ãƒƒã‚¯:** {'âœ…' if DOMAINS_AVAILABLE else 'âŒ'}")
        
        if st.button("çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"):
            render_statistics()
    
    # ãƒ¡ã‚¤ãƒ³ã‚¿ãƒ–
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ“„ åˆ†æ", "ğŸ’¬ ãƒãƒ£ãƒƒãƒˆ", "ğŸ“Š æ¯”è¼ƒ", "ğŸ“ˆ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰", "ğŸ“‹ å±¥æ­´"])
    
    with tab1:
        render_analysis_tab()
    
    with tab2:
        render_chat_tab()
    
    with tab3:
        render_comparison_tab()
    
    with tab4:
        render_dashboard_tab()
    
    with tab5:
        render_history_tab()

def render_analysis_tab():
    """åˆ†æã‚¿ãƒ–"""
    st.header("ğŸ“„ å¥‘ç´„æ›¸åˆ†æ")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_file = st.file_uploader(
        "å¥‘ç´„æ›¸ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=["txt", "pdf", "doc", "docx"],
        help="PDFã€Wordã€ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œ",
    )
    
    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›
    contract_text = st.text_area(
        "ã¾ãŸã¯ç›´æ¥ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›",
        height=200,
        placeholder="å¥‘ç´„æ›¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã“ã“ã«è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„...",
    )
    
    if uploaded_file:
        contract_text = extract_text_from_file(uploaded_file)
        st.info(f"ğŸ“ {uploaded_file.name} ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ{len(contract_text):,}æ–‡å­—ï¼‰")
    
    # åˆ†æå®Ÿè¡Œ
    if st.button("ğŸ” åˆ†æã‚’å®Ÿè¡Œ", type="primary", disabled=not contract_text):
        with st.spinner("åˆ†æä¸­..."):
            engine = VeritasEngine()
            result = engine.analyze(
                contract_text,
                file_name=uploaded_file.name if uploaded_file else "direct_input.txt",
                domain=st.session_state.selected_domain,
            )
            st.session_state.current_analysis = result
            
            # å±¥æ­´ã«è¿½åŠ 
            st.session_state.analysis_history.append({
                "timestamp": result.timestamp,
                "file_name": result.file_name,
                "risk_score": result.risk_score,
                "issue_count": len(result.issues),
                "contract_type": result.contract_type.value,
            })
        
        # çµæœè¡¨ç¤º
        st.success("âœ… åˆ†æå®Œäº†")
        
        # ã‚µãƒãƒªãƒ¼
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            color = "ğŸ”´" if result.risk_score >= 70 else "ğŸŸ " if result.risk_score >= 40 else "ğŸŸ¢"
            st.metric("ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢", f"{color} {result.risk_score:.0f}/100")
        with col2:
            st.metric("æ¤œå‡ºå•é¡Œæ•°", len(result.issues))
        with col3:
            st.metric("å¥‘ç´„ç¨®åˆ¥", result.contract_type.value)
        with col4:
            st.metric("ã‚¨ãƒ³ã‚¸ãƒ³", f"v{result.engine_version}")
        
        # å•é¡Œä¸€è¦§
        st.markdown("### ğŸš¨ æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ")
        
        # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã§ã‚½ãƒ¼ãƒˆ
        sorted_issues = sorted(
            result.issues,
            key=lambda x: [RiskLevel.CRITICAL, RiskLevel.HIGH, RiskLevel.MEDIUM, RiskLevel.LOW, RiskLevel.SAFE].index(x.risk_level),
        )
        
        for issue in sorted_issues:
            render_issue_card(issue)
        
        # ToDoä¸€è¦§
        if result.compressed_todos:
            st.markdown("### âœ… ToDo ãƒªã‚¹ãƒˆï¼ˆåœ§ç¸®æ¸ˆã¿ï¼‰")
            for group in result.compressed_todos:
                with st.expander(f"ğŸ“ {group['group']}ï¼ˆ{len(group['items'])}ä»¶ï¼‰"):
                    for item in group['items']:
                        st.checkbox(item.get('action', str(item)), key=f"todo_{item.get('id', '')}")
        
        # ãƒªãƒ©ã‚¤ãƒˆææ¡ˆ
        if result.rewrite_suggestions:
            st.markdown("### âœï¸ ä¿®æ­£ææ¡ˆ")
            for i, suggestion in enumerate(result.rewrite_suggestions[:5]):
                with st.expander(f"ææ¡ˆ {i+1}"):
                    st.markdown("**åŸæ–‡:**")
                    st.code(suggestion['original'])
                    st.markdown("**ä¿®æ­£æ¡ˆ:**")
                    st.code(suggestion['suggested'])
                    st.markdown(f"**ç†ç”±:** {suggestion['reason']}")
        
        # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        st.markdown("### ğŸ“¥ ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›")
        col1, col2 = st.columns(2)
        
        with col1:
            csv_data = generate_csv_report(result)
            st.download_button(
                "ğŸ“Š CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                csv_data,
                file_name=f"veritas_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv",
            )
        
        with col2:
            word_data = generate_word_report(result)
            st.download_button(
                "ğŸ“ Wordãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                word_data,
                file_name=f"veritas_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            )
        
        # Slacké€šçŸ¥
        if st.session_state.slack_webhook_url and result.risk_score >= 50:
            message = f"ğŸš¨ VERITAS Alert: {result.file_name}\nãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢: {result.risk_score:.0f}/100\nå•é¡Œæ•°: {len(result.issues)}ä»¶"
            if send_slack_notification(st.session_state.slack_webhook_url, message):
                st.success("ğŸ“¢ Slacké€šçŸ¥ã‚’é€ä¿¡ã—ã¾ã—ãŸ")

def render_chat_tab():
    """ãƒãƒ£ãƒƒãƒˆã‚¿ãƒ–"""
    st.header("ğŸ’¬ å¥‘ç´„æ›¸ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ")
    
    if not st.session_state.openai_api_key:
        st.warning("ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§OpenAI APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        return
    
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¡¨ç¤º
    for msg in st.session_state.chat_history:
        with st.chat_message(msg.role):
            st.write(msg.content)
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
    user_input = st.chat_input("å¥‘ç´„æ›¸ã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„...")
    
    if user_input:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        st.session_state.chat_history.append(ChatMessage(role="user", content=user_input))
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
        context = ""
        if st.session_state.current_analysis:
            result = st.session_state.current_analysis
            context = f"""
ç¾åœ¨åˆ†æä¸­ã®å¥‘ç´„æ›¸æƒ…å ±:
- ãƒ•ã‚¡ã‚¤ãƒ«å: {result.file_name}
- å¥‘ç´„ç¨®åˆ¥: {result.contract_type.value}
- ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢: {result.risk_score:.0f}/100
- æ¤œå‡ºå•é¡Œæ•°: {len(result.issues)}ä»¶

ä¸»ãªå•é¡Œ:
"""
            for issue in result.issues[:5]:
                context += f"- [{issue.risk_level.value}] {issue.category}: {issue.description}\n"
        
        prompt = f"""
{context}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {user_input}

æ—¥æœ¬ã®å¥‘ç´„æ›¸æ³•å‹™ã®å°‚é–€å®¶ã¨ã—ã¦ã€ä¸Šè¨˜ã®è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
"""
        
        # AIå¿œç­”ã‚’å–å¾—
        with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­..."):
            response = call_openai_chat(prompt, st.session_state.openai_api_key)
        
        # AIå¿œç­”ã‚’è¿½åŠ 
        st.session_state.chat_history.append(ChatMessage(role="assistant", content=response))
        
        st.rerun()

def render_comparison_tab():
    """æ¯”è¼ƒåˆ†æã‚¿ãƒ–"""
    st.header("ğŸ“Š å¥‘ç´„æ›¸æ¯”è¼ƒåˆ†æ")
    st.info("2ã¤ã®å¥‘ç´„æ›¸ã‚’æ¯”è¼ƒã—ã¦ã€ãƒªã‚¹ã‚¯ã®é•ã„ã‚’åˆ†æã—ã¾ã™ã€‚")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**å¥‘ç´„æ›¸1**")
        contract1 = st.text_area("å¥‘ç´„æ›¸1ã®ãƒ†ã‚­ã‚¹ãƒˆ", height=200, key="compare_contract1")
    
    with col2:
        st.markdown("**å¥‘ç´„æ›¸2**")
        contract2 = st.text_area("å¥‘ç´„æ›¸2ã®ãƒ†ã‚­ã‚¹ãƒˆ", height=200, key="compare_contract2")
    
    if st.button("ğŸ” æ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œ", type="primary"):
        if contract1.strip() and contract2.strip():
            with st.spinner("æ¯”è¼ƒåˆ†æä¸­..."):
                engine = VeritasEngine()
                result1 = engine.analyze(contract1, file_name="å¥‘ç´„æ›¸1")
                result2 = engine.analyze(contract2, file_name="å¥‘ç´„æ›¸2")
            
            st.markdown("---")
            st.markdown("### ğŸ“ˆ æ¯”è¼ƒçµæœ")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("å¥‘ç´„æ›¸1 ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢", f"{result1.risk_score:.0f}ç‚¹")
            with col2:
                st.metric("å¥‘ç´„æ›¸2 ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢", f"{result2.risk_score:.0f}ç‚¹")
            with col3:
                diff = result1.risk_score - result2.risk_score
                st.metric("ã‚¹ã‚³ã‚¢å·®", f"{diff:+.0f}ç‚¹")
            
            # å•é¡Œæ•°æ¯”è¼ƒ
            st.markdown("#### å•é¡Œæ•°æ¯”è¼ƒ")
            data = {
                "é …ç›®": ["CRITICAL", "HIGH", "MEDIUM", "LOW"],
                "å¥‘ç´„æ›¸1": [
                    sum(1 for i in result1.issues if i.risk_level == RiskLevel.CRITICAL),
                    sum(1 for i in result1.issues if i.risk_level == RiskLevel.HIGH),
                    sum(1 for i in result1.issues if i.risk_level == RiskLevel.MEDIUM),
                    sum(1 for i in result1.issues if i.risk_level == RiskLevel.LOW),
                ],
                "å¥‘ç´„æ›¸2": [
                    sum(1 for i in result2.issues if i.risk_level == RiskLevel.CRITICAL),
                    sum(1 for i in result2.issues if i.risk_level == RiskLevel.HIGH),
                    sum(1 for i in result2.issues if i.risk_level == RiskLevel.MEDIUM),
                    sum(1 for i in result2.issues if i.risk_level == RiskLevel.LOW),
                ],
            }
            st.dataframe(data)
        else:
            st.error("ä¸¡æ–¹ã®å¥‘ç´„æ›¸ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

def render_dashboard_tab():
    """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚¿ãƒ–"""
    st.header("ğŸ“ˆ åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
    
    history = st.session_state.analysis_history
    
    if not history:
        st.info("ã¾ã åˆ†æå±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã€Œåˆ†æã€ã‚¿ãƒ–ã§å¥‘ç´„æ›¸ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚")
        return
    
    # æ¦‚è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    total = len(history)
    scores = [h.get("risk_score", 0) for h in history]
    avg_score = sum(scores) / total if total > 0 else 0
    high_risk = sum(1 for s in scores if s >= 50)
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ç·åˆ†ææ•°", total)
    with col2:
        st.metric("å¹³å‡ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢", f"{avg_score:.1f}ç‚¹")
    with col3:
        st.metric("é«˜ãƒªã‚¹ã‚¯ä»¶æ•°", high_risk)
    with col4:
        rate = (high_risk / total * 100) if total > 0 else 0
        st.metric("é«˜ãƒªã‚¹ã‚¯ç‡", f"{rate:.1f}%")
    
    # ãƒˆãƒ¬ãƒ³ãƒ‰ã‚°ãƒ©ãƒ•
    if len(scores) > 1:
        st.markdown("### ğŸ“ˆ ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢æ¨ç§»")
        st.line_chart(scores)
    
    # å¥‘ç´„ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ
    st.markdown("### ğŸ“Š å¥‘ç´„ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ")
    type_counts = defaultdict(int)
    for h in history:
        type_counts[h.get("contract_type", "unknown")] += 1
    
    for contract_type, count in type_counts.items():
        st.progress(count / max(total, 1), text=f"{contract_type}: {count}ä»¶")

def render_history_tab():
    """å±¥æ­´ã‚¿ãƒ–"""
    st.header("ğŸ“‹ åˆ†æå±¥æ­´")
    
    history = st.session_state.analysis_history
    
    if not history:
        st.info("ã¾ã åˆ†æå±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    
    # å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
    for i, h in enumerate(reversed(history)):
        col1, col2, col3, col4 = st.columns([3, 2, 1, 1])
        with col1:
            st.write(h.get("file_name", "unknown"))
        with col2:
            st.write(h.get("timestamp", "")[:19])
        with col3:
            score = h.get("risk_score", 0)
            color = "ğŸ”´" if score >= 70 else "ğŸŸ " if score >= 40 else "ğŸŸ¢"
            st.write(f"{color} {score:.0f}")
        with col4:
            st.write(f"{h.get('issue_count', 0)}ä»¶")
    
    if st.button("ğŸ—‘ï¸ å±¥æ­´ã‚’ã‚¯ãƒªã‚¢"):
        st.session_state.analysis_history = []
        st.rerun()

# =============================================================================
# ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
# =============================================================================

if __name__ == "__main__":
    main()
