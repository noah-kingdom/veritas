#!/usr/bin/env python3
"""
VERITAS v167 - AIå¥‘ç´„æ›¸ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¨ãƒ³ã‚¸ãƒ³ã€å®Œå…¨çµ±åˆç‰ˆã€‘
====================================================================
Patent: 2025-159636 ã€Œå˜˜ãªãã€èª‡å¼µãªãã€éä¸è¶³ãªãã€

â–  v167 æ–°æ©Ÿèƒ½:
ã€v163å¼è­·å£«æ€è€ƒåˆ†è§£ã€‘æ›–æ˜§æ€§æ¤œå‡º/æ¡é …æ•´åˆæ€§/æœŸé–“æœªå®šç¾©æ¤œå‡º
  â†’ å¼è­·å£«æŒ‡æ‘˜6/6é …ç›®(100%)è‡ªå‹•æ¤œå‡ºé”æˆ

â–  Phase 4 æ©Ÿèƒ½:
ã€SMTã‚¨ãƒ³ã‚¸ãƒ³ã€‘å½¢å¼çš„è«–ç†æ¤œè¨¼ï¼ˆZ3äº’æ›ï¼‰
ã€å‘½é¡Œå‡¦ç†éƒ¨ã€‘å¥‘ç´„æ¡é …â†’ä¸€éšè¿°èªè«–ç†(FOL)å¤‰æ›
ã€å½¢å¼æ¤œè¨¼éƒ¨ã€‘å……è¶³å¯èƒ½æ€§åˆ¤å®š(SAT/UNSAT) + ä¸å……è¶³ã‚³ã‚¢æŠ½å‡º
ã€PCRã‚¨ãƒ³ã‚¸ãƒ³ã€‘è¨¼æ˜ä»˜ãä¿®æ­£æ¡ˆ(Proof-Carrying Redlines)ç”Ÿæˆ
ã€CALRçµ±åˆã€‘ã‚³ãƒ³ãƒ•ã‚©ãƒ¼ãƒãƒ«äºˆæ¸¬ã«ã‚ˆã‚‹ä¿¡é ¼åŒºé–“ç®—å‡º
"""

import streamlit as st
import re
import json
import io
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Tuple, Set
from enum import Enum
from datetime import datetime
import hashlib

# ã‚³ã‚¢ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
try:
    from core import unified_pattern_engine, quick_analyze, compress_todos
    CORE_AVAILABLE = True
except ImportError:
    CORE_AVAILABLE = False

# v163å¼è­·å£«æ€è€ƒåˆ†è§£ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
try:
    from core.lawyer_thinking import (
        analyze_ambiguity, AmbiguityType, format_ambiguity_output,
        analyze_contract_coherence, format_coherence_output,
        analyze_contract_time_limits, format_time_limit_output
    )
    LAWYER_THINKING_AVAILABLE = True
except ImportError:
    LAWYER_THINKING_AVAILABLE = False

# Z3ã‚½ãƒ«ãƒãƒ¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    from z3 import Solver, Int, Real, Bool, And, Or, Not, Implies, sat, unsat, unknown
    Z3_AVAILABLE = True
except ImportError:
    Z3_AVAILABLE = False

st.set_page_config(page_title="VERITAS v166ã€Phase 4ã€‘", page_icon="ğŸ”", layout="wide", initial_sidebar_state="expanded")

# =============================================================================
# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹
# =============================================================================
def init_session_state():
    defaults = {
        "analysis_history": [], "chat_history": [], "current_contract": "", "current_analysis": None,
        "user_mode": "staff", "risk_tolerance": "balanced", "specialist_type": "auto",
        "truth_result": None, "ai_consistency_result": None, "ai_answer": "",
        "smt_result": None, "pcr_result": None,
    }
    for k, v in defaults.items():
        if k not in st.session_state:
            st.session_state[k] = v
init_session_state()

RISK_PROFILES = {
    "conservative": {"name": "ä¿å®ˆçš„", "icon": "ğŸ›¡ï¸", "desc": "ãƒªã‚¹ã‚¯æœ€å°åŒ–", "sensitivity": 1.5},
    "cautious": {"name": "æ…é‡", "icon": "âš ï¸", "desc": "å®‰å…¨é‡è¦–", "sensitivity": 1.2},
    "balanced": {"name": "ãƒãƒ©ãƒ³ã‚¹", "icon": "âš–ï¸", "desc": "æ¨™æº–è¨­å®š", "sensitivity": 1.0},
    "aggressive": {"name": "ç©æ¥µçš„", "icon": "ğŸš€", "desc": "åŠ¹ç‡é‡è¦–", "sensitivity": 0.8},
    "maximum": {"name": "æœ€å¤§è¨±å®¹", "icon": "âš¡", "desc": "ã‚¹ãƒ”ãƒ¼ãƒ‰é‡è¦–", "sensitivity": 0.6},
}

class RiskLevel(Enum):
    CRITICAL = "CRITICAL"
    HIGH = "HIGH"
    MEDIUM = "MEDIUM"
    LOW = "LOW"
    SAFE = "SAFE"

class ContractType(Enum):
    NDA = "nda"
    OUTSOURCING = "outsourcing"
    TOS = "tos"
    EMPLOYMENT = "employment"
    GENERAL = "general"

class TruthStatus(Enum):
    SUPPORTED = "supported"
    CONTRADICTED = "contradicted"
    UNSUPPORTED = "unsupported"

class SMTResult(Enum):
    SAT = "SAT"           # å……è¶³å¯èƒ½ï¼ˆçŸ›ç›¾ãªã—ï¼‰
    UNSAT = "UNSAT"       # å……è¶³ä¸èƒ½ï¼ˆçŸ›ç›¾ã‚ã‚Šï¼‰
    UNKNOWN = "UNKNOWN"   # åˆ¤å®šä¸èƒ½

class ContradictionType(Enum):
    DIRECT = "direct"           # P âˆ§ Â¬P
    NUMERIC = "numeric"         # X=a âˆ§ X=b (aâ‰ b)
    QUANTIFIER = "quantifier"   # âˆ€xP(x) âˆ§ âˆƒxÂ¬P(x)
    DIRECTION = "direction"     # Direction(X)>0 âˆ§ Direction(X)<0

@dataclass
class Issue:
    issue_id: str
    clause_text: str
    issue_type: str
    risk_level: RiskLevel
    description: str
    legal_basis: str
    fix_suggestion: str
    category: str = ""
    confidence: float = 0.95
    proof_id: str = ""  # SMTè¨¼æ˜ID

@dataclass
class AnalysisResult:
    issues: List[Issue]
    risk_score: float
    confidence_interval: Tuple[float, float]
    contract_type: ContractType
    specialist_result: Optional[Dict] = None
    truth_result: Optional[Dict] = None
    smt_result: Optional[Dict] = None
    pcr_suggestions: List[Dict] = field(default_factory=list)
    timestamp: str = ""
    file_name: str = ""
    engine_version: str = "1.66.0"
    
    def __post_init__(self):
        if not self.timestamp:
            self.timestamp = datetime.now().isoformat()

# =============================================================================
# äº‹å®ŸDBï¼ˆPhase 3ç¶™æ‰¿ï¼‰
# =============================================================================
FACT_DATABASE = {
    "æœ€ä½è³ƒé‡‘_å…¨å›½åŠ é‡å¹³å‡": {"value": 1004, "unit": "å††/æ™‚é–“", "source": "åšç”ŸåŠ´åƒçœ"},
    "æœ€ä½è³ƒé‡‘_æ±äº¬": {"value": 1163, "unit": "å††/æ™‚é–“", "source": "åšç”ŸåŠ´åƒçœ"},
    "æ³•å®šåŠ´åƒæ™‚é–“_é€±": {"value": 40, "unit": "æ™‚é–“", "source": "åŠ´åƒåŸºæº–æ³•32æ¡"},
    "æ³•å®šåŠ´åƒæ™‚é–“_æ—¥": {"value": 8, "unit": "æ™‚é–“", "source": "åŠ´åƒåŸºæº–æ³•32æ¡"},
    "æ™‚é–“å¤–å‰²å¢—ç‡_é€šå¸¸": {"value": 25, "unit": "%", "source": "åŠ´åƒåŸºæº–æ³•37æ¡"},
    "æ™‚é–“å¤–å‰²å¢—ç‡_60æ™‚é–“è¶…": {"value": 50, "unit": "%", "source": "åŠ´åƒåŸºæº–æ³•37æ¡"},
    "ä¸‹è«‹æ³•æ”¯æ‰•æœŸé™": {"value": 60, "unit": "æ—¥", "source": "ä¸‹è«‹æ³•4æ¡1é …2å·"},
    "åˆ©æ¯åˆ¶é™æ³•_100ä¸‡å††ä»¥ä¸Š": {"value": 15, "unit": "%", "source": "åˆ©æ¯åˆ¶é™æ³•1æ¡"},
    "é…å»¶æå®³é‡‘ä¸Šé™_æ¶ˆè²»è€…": {"value": 14.6, "unit": "%", "source": "æ¶ˆè²»è€…å¥‘ç´„æ³•9æ¡2å·"},
    "è§£é›‡äºˆå‘ŠæœŸé–“": {"value": 30, "unit": "æ—¥", "source": "åŠ´åƒåŸºæº–æ³•20æ¡"},
    "ã‚¯ãƒ¼ãƒªãƒ³ã‚°ã‚ªãƒ•æœŸé–“_è¨ªå•è²©å£²": {"value": 8, "unit": "æ—¥", "source": "ç‰¹å•†æ³•9æ¡"},
    "æ¶ˆè²»ç¨ç‡": {"value": 10, "unit": "%", "source": "æ¶ˆè²»ç¨æ³•"},
}

# =============================================================================
# æ³•ä»¤ãƒ«ãƒ¼ãƒ«DBï¼ˆSMTå…¬ç†ç”¨ï¼‰
# =============================================================================
LEGAL_AXIOMS = {
    "æ°‘æ³•709æ¡": {
        "name": "ä¸æ³•è¡Œç‚ºè²¬ä»»",
        "axiom": "âˆ€x(Tort(x) â†’ Liability(x))",
        "description": "æ•…æ„åˆã¯éå¤±ã«ã‚ˆã£ã¦ä»–äººã®æ¨©åˆ©ã‚’ä¾µå®³ã—ãŸè€…ã¯æå®³è³ å„Ÿè²¬ä»»ã‚’è² ã†",
    },
    "æ¶ˆè²»è€…å¥‘ç´„æ³•8æ¡1é …1å·": {
        "name": "å…¨éƒ¨å…è²¬ç„¡åŠ¹",
        "axiom": "Â¬âˆ€x(Consumer(x) â†’ TotalExemption(x))",
        "description": "äº‹æ¥­è€…ã®å‚µå‹™ä¸å±¥è¡Œã«ã‚ˆã‚‹æå®³è³ å„Ÿè²¬ä»»ã®å…¨éƒ¨ã‚’å…é™¤ã™ã‚‹æ¡é …ã¯ç„¡åŠ¹",
    },
    "æ¶ˆè²»è€…å¥‘ç´„æ³•8æ¡1é …2å·": {
        "name": "æ•…æ„é‡éå¤±å…è²¬ç„¡åŠ¹",
        "axiom": "Â¬âˆ€x(GrossNegligence(x) â†’ Exemption(x))",
        "description": "æ•…æ„åˆã¯é‡å¤§ãªéå¤±ã«ã‚ˆã‚‹æå®³è³ å„Ÿè²¬ä»»ã®ä¸€éƒ¨ã‚’å…é™¤ã™ã‚‹æ¡é …ã¯ç„¡åŠ¹",
    },
    "ä¸‹è«‹æ³•4æ¡1é …2å·": {
        "name": "æ”¯æ‰•é…å»¶ç¦æ­¢",
        "axiom": "âˆ€x(Payment(x) â†’ PaymentDays(x) â‰¤ 60)",
        "description": "å—é ˜æ—¥ã‹ã‚‰60æ—¥ä»¥å†…ã«æ”¯æ‰•ã‚ãªã‘ã‚Œã°ãªã‚‰ãªã„",
    },
    "åŠ´åƒåŸºæº–æ³•16æ¡": {
        "name": "è³ å„Ÿäºˆå®šç¦æ­¢",
        "axiom": "Â¬âˆƒx(Employee(x) âˆ§ PenaltyPredetermined(x))",
        "description": "åŠ´åƒå¥‘ç´„ã®ä¸å±¥è¡Œã«ã¤ã„ã¦é•ç´„é‡‘ã‚’å®šã‚ã¦ã¯ãªã‚‰ãªã„",
    },
    "åŠ´åƒåŸºæº–æ³•20æ¡": {
        "name": "è§£é›‡äºˆå‘Š",
        "axiom": "âˆ€x(Dismissal(x) â†’ NoticeDays(x) â‰¥ 30)",
        "description": "è§£é›‡ã¯å°‘ãªãã¨ã‚‚30æ—¥å‰ã«äºˆå‘Šã—ãªã‘ã‚Œã°ãªã‚‰ãªã„",
    },
    "å€Ÿåœ°å€Ÿå®¶æ³•30æ¡": {
        "name": "å€Ÿå®¶äººä¸åˆ©ç‰¹ç´„ç„¡åŠ¹",
        "axiom": "Â¬âˆ€x(Tenant(x) â†’ UnfavorableClause(x))",
        "description": "å€Ÿå®¶äººã«ä¸åˆ©ãªç‰¹ç´„ã¯ç„¡åŠ¹",
    },
}

# =============================================================================
# SMTã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆå½¢å¼æ¤œè¨¼éƒ¨ï¼‰
# =============================================================================

class Proposition:
    """å‘½é¡Œã‚¯ãƒ©ã‚¹"""
    def __init__(self, prop_id: str, text: str, prop_type: str, subject: str = "", predicate: str = "", value: Any = None):
        self.prop_id = prop_id
        self.text = text
        self.prop_type = prop_type  # state, quantifier, numeric, direction
        self.subject = subject
        self.predicate = predicate
        self.value = value
        self.negated = False
    
    def negate(self):
        self.negated = not self.negated
        return self
    
    def to_fol(self) -> str:
        """ä¸€éšè¿°èªè«–ç†å¼ã«å¤‰æ›"""
        neg = "Â¬" if self.negated else ""
        if self.prop_type == "state":
            return f"{neg}{self.predicate}({self.subject})"
        elif self.prop_type == "numeric":
            op = "=" if not self.negated else "â‰ "
            return f"{self.subject} {op} {self.value}"
        elif self.prop_type == "direction":
            op = ">" if not self.negated else "â‰¤"
            return f"Direction({self.subject}) {op} 0"
        elif self.prop_type == "quantifier":
            q = "âˆ€" if not self.negated else "âˆƒ"
            return f"{q}x({self.predicate}(x))"
        return f"{neg}P_{self.prop_id}"


class PropositionExtractor:
    """å‘½é¡ŒæŠ½å‡ºéƒ¨"""
    
    PATTERNS = [
        # çŠ¶æ…‹å‘½é¡Œ
        {"pattern": r"(.{2,10})ã¯(.{2,15})ã§ã‚ã‚‹", "type": "state", "groups": ("subject", "predicate")},
        {"pattern": r"(.{2,10})ã¯(.{2,15})ã§ãªã„", "type": "state", "groups": ("subject", "predicate"), "negated": True},
        {"pattern": r"(.{2,10})ãŒ(.{2,15})ã™ã‚‹", "type": "state", "groups": ("subject", "predicate")},
        # æ•°å€¤å‘½é¡Œ
        {"pattern": r"(.{2,15})ã¯(\d+\.?\d*)\s*(å††|%|æ—¥|å¹´|æ™‚é–“|ãƒ¶æœˆ)", "type": "numeric", "groups": ("subject", "value", "unit")},
        {"pattern": r"(.{2,15})ã®(ä¸Šé™|ä¸‹é™|æœ€å¤§|æœ€å°)ã¯(\d+\.?\d*)", "type": "numeric", "groups": ("subject", "bound", "value")},
        # æ–¹å‘æ€§å‘½é¡Œ
        {"pattern": r"(.{2,10})ã¯(å¢—åŠ |ä¸Šæ˜‡|æ‹¡å¤§)", "type": "direction", "groups": ("subject",), "positive": True},
        {"pattern": r"(.{2,10})ã¯(æ¸›å°‘|ä¸‹è½|ç¸®å°)", "type": "direction", "groups": ("subject",), "positive": False},
        # é‡åŒ–å‘½é¡Œ
        {"pattern": r"(å…¨ã¦|ã™ã¹ã¦|ä¸€åˆ‡)ã®(.{2,10})ãŒ(.{2,15})", "type": "quantifier", "groups": ("_", "subject", "predicate"), "universal": True},
        {"pattern": r"(ä¸€éƒ¨|éƒ¨åˆ†çš„)ã®(.{2,10})ãŒ(.{2,15})", "type": "quantifier", "groups": ("_", "subject", "predicate"), "universal": False},
    ]
    
    @classmethod
    def extract(cls, text: str) -> List[Proposition]:
        propositions = []
        prop_counter = 0
        
        for pinfo in cls.PATTERNS:
            for match in re.finditer(pinfo["pattern"], text, re.I):
                prop_counter += 1
                prop_id = f"P{prop_counter:03d}"
                
                if pinfo["type"] == "state":
                    subject = match.group(1).strip()
                    predicate = match.group(2).strip()
                    prop = Proposition(prop_id, match.group(), "state", subject, predicate)
                    if pinfo.get("negated"):
                        prop.negate()
                
                elif pinfo["type"] == "numeric":
                    subject = match.group(1).strip()
                    value = float(match.group(2))
                    prop = Proposition(prop_id, match.group(), "numeric", subject, value=value)
                
                elif pinfo["type"] == "direction":
                    subject = match.group(1).strip()
                    prop = Proposition(prop_id, match.group(), "direction", subject)
                    if not pinfo.get("positive"):
                        prop.negate()
                
                elif pinfo["type"] == "quantifier":
                    subject = match.group(2).strip()
                    predicate = match.group(3).strip()
                    prop = Proposition(prop_id, match.group(), "quantifier", subject, predicate)
                    if not pinfo.get("universal"):
                        prop.negate()
                
                else:
                    continue
                
                propositions.append(prop)
        
        return propositions


class SMTEngine:
    """SMTã‚½ãƒ«ãƒãƒ¼ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆå½¢å¼æ¤œè¨¼éƒ¨ï¼‰"""
    
    @classmethod
    def verify(cls, propositions: List[Proposition], text: str = "") -> Dict[str, Any]:
        """
        å‘½é¡Œé›†åˆã®å……è¶³å¯èƒ½æ€§ã‚’æ¤œè¨¼
        Returns: {result: SAT/UNSAT/UNKNOWN, contradictions: [...], unsat_core: [...]}
        """
        if not propositions:
            return {"result": SMTResult.SAT.value, "contradictions": [], "unsat_core": [], "proof_id": None}
        
        contradictions = []
        unsat_core = []
        
        # 1. ç›´æ¥çŸ›ç›¾ãƒã‚§ãƒƒã‚¯ï¼ˆP âˆ§ Â¬Pï¼‰
        state_props = {}
        for prop in propositions:
            if prop.prop_type == "state":
                key = f"{prop.subject}_{prop.predicate}"
                if key in state_props:
                    other = state_props[key]
                    if other.negated != prop.negated:
                        contradictions.append({
                            "type": ContradictionType.DIRECT.value,
                            "props": [prop.prop_id, other.prop_id],
                            "description": f"ç›´æ¥çŸ›ç›¾: {prop.to_fol()} ã¨ {other.to_fol()}",
                            "severity": "CRITICAL",
                        })
                        unsat_core.extend([prop.prop_id, other.prop_id])
                else:
                    state_props[key] = prop
        
        # 2. æ•°å€¤çŸ›ç›¾ãƒã‚§ãƒƒã‚¯ï¼ˆX=a âˆ§ X=b where aâ‰ bï¼‰
        numeric_props = {}
        for prop in propositions:
            if prop.prop_type == "numeric":
                key = prop.subject
                if key in numeric_props:
                    other = numeric_props[key]
                    if other.value != prop.value:
                        contradictions.append({
                            "type": ContradictionType.NUMERIC.value,
                            "props": [prop.prop_id, other.prop_id],
                            "description": f"æ•°å€¤çŸ›ç›¾: {prop.subject}={prop.value} ã¨ {other.subject}={other.value}",
                            "severity": "HIGH",
                        })
                        unsat_core.extend([prop.prop_id, other.prop_id])
                else:
                    numeric_props[key] = prop
        
        # 3. æ–¹å‘æ€§çŸ›ç›¾ãƒã‚§ãƒƒã‚¯ï¼ˆå¢—åŠ  âˆ§ æ¸›å°‘ï¼‰
        direction_props = {}
        for prop in propositions:
            if prop.prop_type == "direction":
                key = prop.subject
                if key in direction_props:
                    other = direction_props[key]
                    if other.negated != prop.negated:
                        contradictions.append({
                            "type": ContradictionType.DIRECTION.value,
                            "props": [prop.prop_id, other.prop_id],
                            "description": f"æ–¹å‘æ€§çŸ›ç›¾: {prop.subject}ã®å¢—åŠ ã¨æ¸›å°‘ãŒåŒæ™‚ã«è¨˜è¼‰",
                            "severity": "HIGH",
                        })
                        unsat_core.extend([prop.prop_id, other.prop_id])
                else:
                    direction_props[key] = prop
        
        # 4. é‡åŒ–çŸ›ç›¾ãƒã‚§ãƒƒã‚¯ï¼ˆâˆ€xP(x) âˆ§ âˆƒxÂ¬P(x)ï¼‰
        quant_props = {}
        for prop in propositions:
            if prop.prop_type == "quantifier":
                key = f"{prop.subject}_{prop.predicate}"
                if key in quant_props:
                    other = quant_props[key]
                    if other.negated != prop.negated:
                        contradictions.append({
                            "type": ContradictionType.QUANTIFIER.value,
                            "props": [prop.prop_id, other.prop_id],
                            "description": f"é‡åŒ–çŸ›ç›¾: å…¨ç§°ã¨å­˜åœ¨ã®çŸ›ç›¾",
                            "severity": "MEDIUM",
                        })
                        unsat_core.extend([prop.prop_id, other.prop_id])
                else:
                    quant_props[key] = prop
        
        # 5. æ³•ä»¤å…¬ç†ã¨ã®çŸ›ç›¾ãƒã‚§ãƒƒã‚¯
        legal_violations = cls._check_legal_axioms(propositions, text)
        contradictions.extend(legal_violations)
        
        # çµæœåˆ¤å®š
        if contradictions:
            result = SMTResult.UNSAT
            proof_id = f"PRF-{hashlib.md5(str(contradictions).encode()).hexdigest()[:8].upper()}"
        else:
            result = SMTResult.SAT
            proof_id = None
        
        return {
            "result": result.value,
            "contradictions": contradictions,
            "unsat_core": list(set(unsat_core)),
            "unsat_core_size": len(set(unsat_core)),
            "proof_id": proof_id,
            "propositions_count": len(propositions),
            "fol_formulas": [p.to_fol() for p in propositions[:10]],  # æœ€åˆã®10å€‹
        }
    
    @classmethod
    def _check_legal_axioms(cls, propositions: List[Proposition], text: str) -> List[Dict]:
        """æ³•ä»¤å…¬ç†ã¨ã®çŸ›ç›¾ãƒã‚§ãƒƒã‚¯"""
        violations = []
        
        # å…¨éƒ¨å…è²¬ãƒã‚§ãƒƒã‚¯
        if re.search(r"ä¸€åˆ‡.{0,10}(è²¬ä»»|è³ å„Ÿ).{0,10}(è² ã‚ãªã„|å…é™¤|ãªã—)", text, re.I):
            violations.append({
                "type": "LEGAL_VIOLATION",
                "axiom": "æ¶ˆè²»è€…å¥‘ç´„æ³•8æ¡1é …1å·",
                "description": "å…¨éƒ¨å…è²¬æ¡é …ã¯æ¶ˆè²»è€…å¥‘ç´„æ³•8æ¡1é …1å·ã«é•åã™ã‚‹å¯èƒ½æ€§",
                "severity": "CRITICAL",
            })
        
        # æ”¯æ‰•æœŸé™ãƒã‚§ãƒƒã‚¯
        payment_match = re.search(r"æ”¯æ‰•.{0,20}(\d+)\s*æ—¥", text, re.I)
        if payment_match:
            days = int(payment_match.group(1))
            if days > 60:
                violations.append({
                    "type": "LEGAL_VIOLATION",
                    "axiom": "ä¸‹è«‹æ³•4æ¡1é …2å·",
                    "description": f"æ”¯æ‰•æœŸé™{days}æ—¥ã¯ä¸‹è«‹æ³•ã®60æ—¥è¦åˆ¶ã«é•å",
                    "severity": "CRITICAL",
                })
        
        # è§£é›‡äºˆå‘Šãƒã‚§ãƒƒã‚¯
        notice_match = re.search(r"(è§£é›‡|é€€è·).{0,10}(\d+)\s*æ—¥å‰.{0,10}(äºˆå‘Š|é€šçŸ¥)", text, re.I)
        if notice_match:
            days = int(notice_match.group(2))
            if days < 30:
                violations.append({
                    "type": "LEGAL_VIOLATION",
                    "axiom": "åŠ´åƒåŸºæº–æ³•20æ¡",
                    "description": f"è§£é›‡äºˆå‘Š{days}æ—¥ã¯åŠ´åŸºæ³•20æ¡ã®30æ—¥è¦åˆ¶ã«é•å",
                    "severity": "HIGH",
                })
        
        # é•ç´„é‡‘äºˆå®šãƒã‚§ãƒƒã‚¯ï¼ˆåŠ´åƒå¥‘ç´„ï¼‰
        if re.search(r"(åŠ´åƒ|é›‡ç”¨|å¾“æ¥­å“¡).{0,50}(é•ç´„é‡‘|æå®³è³ å„Ÿ.{0,5}äºˆå®š)", text, re.I):
            violations.append({
                "type": "LEGAL_VIOLATION",
                "axiom": "åŠ´åƒåŸºæº–æ³•16æ¡",
                "description": "åŠ´åƒå¥‘ç´„ã«ãŠã‘ã‚‹é•ç´„é‡‘äºˆå®šã¯åŠ´åŸºæ³•16æ¡ã«é•å",
                "severity": "CRITICAL",
            })
        
        return violations


class SMTVerifier:
    """SMTæ¤œè¨¼çµ±åˆã‚¯ãƒ©ã‚¹"""
    
    @classmethod
    def analyze(cls, text: str) -> Dict[str, Any]:
        # 1. å‘½é¡ŒæŠ½å‡º
        propositions = PropositionExtractor.extract(text)
        
        # 2. SMTæ¤œè¨¼
        smt_result = SMTEngine.verify(propositions, text)
        
        # 3. éé©åˆåº¦ã‚¹ã‚³ã‚¢ç®—å‡ºï¼ˆã‚³ãƒ³ãƒ•ã‚©ãƒ¼ãƒãƒ«äºˆæ¸¬ç”¨ï¼‰
        nonconformity_score = cls._calculate_nonconformity(smt_result)
        
        # 4. ä¿¡é ¼åŒºé–“ç®—å‡º
        confidence_interval = cls._calculate_confidence_interval(nonconformity_score)
        
        # 5. çœŸå®Ÿåº¦ã‚¹ã‚³ã‚¢
        truth_score = max(0, 100 - nonconformity_score * 20)
        
        return {
            "smt_result": smt_result["result"],
            "contradictions": smt_result["contradictions"],
            "unsat_core": smt_result["unsat_core"],
            "proof_id": smt_result["proof_id"],
            "propositions_count": smt_result["propositions_count"],
            "fol_formulas": smt_result["fol_formulas"],
            "nonconformity_score": nonconformity_score,
            "truth_score": truth_score,
            "confidence_interval": confidence_interval,
            "grade": "A" if truth_score >= 90 else "B" if truth_score >= 70 else "C" if truth_score >= 50 else "D",
        }
    
    @classmethod
    def _calculate_nonconformity(cls, smt_result: Dict) -> float:
        """éé©åˆåº¦ã‚¹ã‚³ã‚¢ç®—å‡º"""
        severity_weights = {"CRITICAL": 3.0, "HIGH": 2.0, "MEDIUM": 1.0, "LOW": 0.5}
        
        score = 0.0
        for contradiction in smt_result.get("contradictions", []):
            weight = severity_weights.get(contradiction.get("severity", "MEDIUM"), 1.0)
            score += weight
        
        # ä¸å……è¶³ã‚³ã‚¢ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹èª¿æ•´
        core_size = smt_result.get("unsat_core_size", 0)
        score += core_size * 0.5
        
        return min(5.0, score)  # ä¸Šé™5.0
    
    @classmethod
    def _calculate_confidence_interval(cls, nonconformity_score: float) -> Tuple[float, float]:
        """ä¿¡é ¼åŒºé–“ç®—å‡ºï¼ˆã‚³ãƒ³ãƒ•ã‚©ãƒ¼ãƒãƒ«äºˆæ¸¬ï¼‰"""
        base_score = max(0, 100 - nonconformity_score * 20)
        
        # éé©åˆåº¦ã«åŸºã¥ãä¿¡é ¼åŒºé–“å¹…
        if nonconformity_score <= 1.0:
            margin = 5
        elif nonconformity_score <= 2.0:
            margin = 10
        elif nonconformity_score <= 3.0:
            margin = 15
        else:
            margin = 20
        
        return (max(0, base_score - margin), min(100, base_score + margin))


# =============================================================================
# PCRã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆè¨¼æ˜ä»˜ãä¿®æ­£æ¡ˆç”Ÿæˆï¼‰
# =============================================================================

class PCREngine:
    """Proof-Carrying Redlines ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    REDLINE_TEMPLATES = {
        "å…¨éƒ¨å…è²¬": {
            "original_pattern": r"ä¸€åˆ‡.{0,10}(è²¬ä»»|è³ å„Ÿ).{0,10}(è² ã‚ãªã„|å…é™¤)",
            "redline": "æ•…æ„åˆã¯é‡éå¤±ã«ã‚ˆã‚‹å ´åˆã‚’é™¤ãã€ç›´æ¥æå®³ã«é™ã‚Šã€æœ¬å¥‘ç´„ã«åŸºã¥ãå—é ˜ã—ãŸé‡‘é¡ã‚’ä¸Šé™ã¨ã—ã¦è²¬ä»»ã‚’è² ã†",
            "proof": {
                "axiom": "æ¶ˆè²»è€…å¥‘ç´„æ³•8æ¡1é …1å·",
                "verification": "TotalExemption(x) â†’ Â¬Valid(x) ãŒæˆç«‹ã—ãªã„ã“ã¨ã‚’ç¢ºèª",
                "result": "ä¿®æ­£å¾Œã®æ¡é …ã¯å…¨éƒ¨å…è²¬ã«è©²å½“ã—ãªã„",
            },
        },
        "60æ—¥è¶…æ”¯æ‰•": {
            "original_pattern": r"æ”¯æ‰•.{0,20}(6[1-9]|[7-9]\d|1\d{2,})\s*æ—¥",
            "redline": "æ¤œåå®Œäº†æ—¥ã®å±ã™ã‚‹æœˆã®ç¿Œæœˆæœ«æ—¥ï¼ˆ60æ—¥ä»¥å†…ï¼‰ã«æ”¯æ‰•ã†",
            "proof": {
                "axiom": "ä¸‹è«‹æ³•4æ¡1é …2å·",
                "verification": "PaymentDays(x) â‰¤ 60 ãŒæˆç«‹ã™ã‚‹ã“ã¨ã‚’ç¢ºèª",
                "result": "ä¿®æ­£å¾Œã®æ”¯æ‰•æœŸé™ã¯æ³•å®šä¸Šé™ä»¥å†…",
            },
        },
        "è§£é›‡äºˆå‘Šä¸è¶³": {
            "original_pattern": r"(è§£é›‡|é€€è·).{0,10}([1-2]?\d)\s*æ—¥å‰.{0,10}(äºˆå‘Š|é€šçŸ¥)",
            "redline": "è§£é›‡ã™ã‚‹å ´åˆã¯å°‘ãªãã¨ã‚‚30æ—¥å‰ã«äºˆå‘Šã™ã‚‹",
            "proof": {
                "axiom": "åŠ´åƒåŸºæº–æ³•20æ¡",
                "verification": "NoticeDays(x) â‰¥ 30 ãŒæˆç«‹ã™ã‚‹ã“ã¨ã‚’ç¢ºèª",
                "result": "ä¿®æ­£å¾Œã®äºˆå‘ŠæœŸé–“ã¯æ³•å®šä¸‹é™ä»¥ä¸Š",
            },
        },
        "ä¸€æ–¹çš„å¤‰æ›´": {
            "original_pattern": r"(é€šçŸ¥|äºˆå‘Š).{0,10}(ãªã|ãªã—).{0,15}(å¤‰æ›´|æ”¹å®š)",
            "redline": "å¤‰æ›´ã®åŠ¹åŠ›ç™ºç”Ÿæ—¥ã®30æ—¥å‰ã¾ã§ã«å¤‰æ›´å†…å®¹ã‚’é€šçŸ¥ã™ã‚‹",
            "proof": {
                "axiom": "æ°‘æ³•548æ¡ã®4",
                "verification": "NotificationPeriod(x) â‰¥ 30 ãŒæˆç«‹ã™ã‚‹ã“ã¨ã‚’ç¢ºèª",
                "result": "ä¿®æ­£å¾Œã®å¤‰æ›´æ‰‹ç¶šãã¯å®šå‹ç´„æ¬¾å¤‰æ›´ãƒ«ãƒ¼ãƒ«ã«é©åˆ",
            },
        },
        "ç«¶æ¥­é¿æ­¢éå¤§": {
            "original_pattern": r"ç«¶æ¥­.{0,15}([2-9]|1\d)\s*å¹´",
            "redline": "é€€è·å¾Œ6ãƒ¶æœˆé–“ã€åœ¨è·ä¸­ã«æ‹…å½“ã—ãŸæ¥­å‹™ã¨ç›´æ¥ç«¶åˆã™ã‚‹æ¥­å‹™ã¸ã®å¾“äº‹ã‚’åˆ¶é™ã™ã‚‹ã€‚ä»£å„Ÿã¨ã—ã¦åŸºæœ¬çµ¦ã®3ãƒ¶æœˆåˆ†ã‚’æ”¯çµ¦ã™ã‚‹",
            "proof": {
                "axiom": "æ†²æ³•22æ¡ï¼ˆè·æ¥­é¸æŠã®è‡ªç”±ï¼‰",
                "verification": "Duration(x) â‰¤ 1 âˆ§ Compensation(x) ãŒæˆç«‹ã™ã‚‹ã“ã¨ã‚’ç¢ºèª",
                "result": "ä¿®æ­£å¾Œã®ç«¶æ¥­é¿æ­¢ã¯æœŸé–“ãƒ»ä»£å„Ÿæªç½®ã®è¦³ç‚¹ã‹ã‚‰åˆç†çš„",
            },
        },
    }
    
    @classmethod
    def generate(cls, text: str, smt_result: Dict) -> List[Dict[str, Any]]:
        """è¨¼æ˜ä»˜ãä¿®æ­£æ¡ˆã‚’ç”Ÿæˆ"""
        redlines = []
        redline_counter = 0
        
        for key, template in cls.REDLINE_TEMPLATES.items():
            match = re.search(template["original_pattern"], text, re.I)
            if match:
                redline_counter += 1
                proof_id = f"PCR-{datetime.now():%Y%m%d}-{redline_counter:03d}"
                
                redlines.append({
                    "id": proof_id,
                    "issue": key,
                    "original": match.group(),
                    "redline": template["redline"],
                    "proof": {
                        "proof_id": proof_id,
                        "axiom": template["proof"]["axiom"],
                        "verification": template["proof"]["verification"],
                        "result": template["proof"]["result"],
                        "smt_verified": smt_result.get("result") == SMTResult.UNSAT.value,
                    },
                    "position": match.span(),
                })
        
        return redlines


# =============================================================================
# Truth Engineï¼ˆPhase 3ç¶™æ‰¿ + SMTçµ±åˆï¼‰
# =============================================================================

class FactChecker:
    FACT_PATTERNS = [
        {"pattern": r"æœ€ä½è³ƒé‡‘.{0,10}(\d+)\s*å††", "fact_key": "æœ€ä½è³ƒé‡‘_å…¨å›½åŠ é‡å¹³å‡", "type": "numeric"},
        {"pattern": r"æ”¯æ‰•.{0,10}(\d+)\s*æ—¥ä»¥å†…", "fact_key": "ä¸‹è«‹æ³•æ”¯æ‰•æœŸé™", "type": "numeric_max"},
        {"pattern": r"å¹´åˆ©.{0,10}(\d+\.?\d*)\s*(%|ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆ)", "fact_key": "åˆ©æ¯åˆ¶é™æ³•_100ä¸‡å††ä»¥ä¸Š", "type": "numeric_max"},
        {"pattern": r"è§£é›‡.{0,10}(\d+)\s*æ—¥å‰.{0,10}äºˆå‘Š", "fact_key": "è§£é›‡äºˆå‘ŠæœŸé–“", "type": "numeric_min"},
    ]
    
    @classmethod
    def check(cls, text: str) -> List[Dict[str, Any]]:
        issues = []
        for fp in cls.FACT_PATTERNS:
            match = re.search(fp["pattern"], text, re.I)
            if match:
                try:
                    claimed_value = float(match.group(1))
                except:
                    continue
                fact = FACT_DATABASE.get(fp["fact_key"])
                if not fact or fact["value"] is None:
                    continue
                correct_value = fact["value"]
                is_error = False
                if fp["type"] == "numeric" and claimed_value != correct_value:
                    is_error = True
                elif fp["type"] == "numeric_max" and claimed_value > correct_value:
                    is_error = True
                elif fp["type"] == "numeric_min" and claimed_value < correct_value:
                    is_error = True
                if is_error:
                    issues.append({
                        "type": "FACT_ERROR", "category": "äº‹å®Ÿèª¤ã‚Š", "severity": "HIGH",
                        "claimed": f"{claimed_value}{fact['unit']}", "correct": f"{correct_value}{fact['unit']}",
                        "source": fact["source"], "description": f"è¨˜è¼‰å€¤ã€Œ{claimed_value}{fact['unit']}ã€ã¯æ­£ç¢ºãªå€¤ã€Œ{correct_value}{fact['unit']}ã€ã¨ç•°ãªã‚Šã¾ã™",
                    })
        return issues


class LogicChecker:
    LOGIC_PATTERNS = [
        {"id": "LC01", "name": "è²¬ä»»çŸ›ç›¾", "patterns": [r"ä¸€åˆ‡.{0,10}è²¬ä»».{0,10}(è² ã‚ãªã„|å…é™¤).{0,100}æå®³.{0,10}è³ å„Ÿ"], "severity": "CRITICAL"},
        {"id": "LC02", "name": "ç¦æ­¢è¨±å¯çŸ›ç›¾", "patterns": [r"(ç¦æ­¢|ã—ã¦ã¯ãªã‚‰ãªã„).{0,50}(å¯èƒ½|ã§ãã‚‹|èªã‚ã‚‹)"], "severity": "MEDIUM"},
        {"id": "LC03", "name": "å¢—æ¸›çŸ›ç›¾", "patterns": [r"(å£²ä¸Š|åˆ©ç›Š).{0,20}(å¢—åŠ |ä¸Šæ˜‡).{0,50}\1.{0,20}(æ¸›å°‘|ä¸‹è½)"], "severity": "HIGH"},
    ]
    
    @classmethod
    def check(cls, text: str) -> List[Dict[str, Any]]:
        issues = []
        for lp in cls.LOGIC_PATTERNS:
            for pattern in lp["patterns"]:
                if re.search(pattern, text, re.I | re.DOTALL):
                    issues.append({"type": "LOGIC_ERROR", "id": lp["id"], "category": lp["name"], "severity": lp["severity"], "description": f"è«–ç†çŸ›ç›¾: {lp['name']}"})
        return issues


class ContextChecker:
    CONTEXT_PATTERNS = [
        {"id": "CC01", "name": "å…è²¬ã¨ä¿è¨¼ã®çŸ›ç›¾", "condition": r"(ä¿è¨¼|warranti)", "conflict": r"ä¸€åˆ‡.{0,10}è²¬ä»».{0,10}(è² ã‚ãªã„|å…é™¤)", "severity": "CRITICAL"},
        {"id": "CC02", "name": "è§£é™¤æ¨©ã®éå¯¾ç§°", "condition": r"ç”².{0,20}(è§£é™¤ã§ãã‚‹|è§£é™¤æ¨©)", "conflict": r"ä¹™.{0,20}(è§£é™¤ã§ããªã„|è§£é™¤æ¨©.{0,5}ãªã„)", "severity": "HIGH"},
    ]
    
    @classmethod
    def check(cls, text: str) -> List[Dict[str, Any]]:
        issues = []
        for cp in cls.CONTEXT_PATTERNS:
            if re.search(cp["condition"], text, re.I) and re.search(cp["conflict"], text, re.I):
                issues.append({"type": "CONTEXT_ERROR", "id": cp["id"], "category": cp["name"], "severity": cp["severity"], "description": cp["name"]})
        return issues


class TruthEngine:
    @classmethod
    def analyze(cls, text: str) -> Dict[str, Any]:
        fact_issues = FactChecker.check(text)
        logic_issues = LogicChecker.check(text)
        context_issues = ContextChecker.check(text)
        all_issues = fact_issues + logic_issues + context_issues
        penalty = sum({"CRITICAL": 30, "HIGH": 20, "MEDIUM": 10, "LOW": 5}.get(i.get("severity", "MEDIUM"), 10) for i in all_issues)
        truth_score = max(0, 100 - penalty)
        return {
            "truth_score": truth_score, "grade": "A" if truth_score >= 90 else "B" if truth_score >= 70 else "C" if truth_score >= 50 else "D",
            "fact_issues": fact_issues, "logic_issues": logic_issues, "context_issues": context_issues, "total_issues": len(all_issues),
            "breakdown": {"fact": len(fact_issues), "logic": len(logic_issues), "context": len(context_issues)}
        }


# =============================================================================
# å±é™ºãƒ‘ã‚¿ãƒ¼ãƒ³
# =============================================================================
DANGER_PATTERNS = {
    "absolute_waiver": {"patterns": [r"ä¸€åˆ‡.{0,10}(è²¬ä»»|è³ å„Ÿ).{0,10}(è² |ã—)?ãªã„"], "risk": RiskLevel.CRITICAL, "category": "å…è²¬æ¡é …",
        "description": "ä¸€åˆ‡ã®è²¬ä»»ã‚’å…é™¤ã™ã‚‹æ¡é …", "legal_basis": "æ¶ˆè²»è€…å¥‘ç´„æ³•ç¬¬8æ¡", "fix": "ã€Œæ•…æ„é‡éå¤±ã‚’é™¤ãã€ç­‰ã®é™å®šè¿½åŠ "},
    "payment_over_60days": {"patterns": [r"æ”¯æ‰•.{0,20}(6[1-9]|[7-9]\d|1\d{2,})\s*æ—¥"], "risk": RiskLevel.CRITICAL, "category": "æ”¯æ‰•é…å»¶",
        "description": "60æ—¥è¶…ã®æ”¯æ‰•æœŸæ—¥", "legal_basis": "ä¸‹è«‹æ³•ç¬¬4æ¡1é …2å·", "fix": "60æ—¥ä»¥å†…ã«ä¿®æ­£"},
    "disguised_employment": {"patterns": [r"(æ¥­å‹™å§”è¨—|è«‹è² ).{0,30}(æŒ‡æ®å‘½ä»¤|å‡ºé€€å‹¤.{0,5}ç®¡ç†)"], "risk": RiskLevel.CRITICAL, "category": "å½è£…è«‹è² ",
        "description": "æ¥­å‹™å§”è¨—ã®å®Ÿæ…‹ãŒé›‡ç”¨", "legal_basis": "åŠ´åƒåŸºæº–æ³•", "fix": "å¥‘ç´„å½¢æ…‹ã®è¦‹ç›´ã—"},
}


# =============================================================================
# ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ã‚¸ãƒ³
# =============================================================================
class VeritasEngine:
    VERSION = "1.66.0"
    
    def __init__(self, risk_tolerance: str = "balanced"):
        self.issue_counter = 0
        self.sensitivity = RISK_PROFILES.get(risk_tolerance, RISK_PROFILES["balanced"])["sensitivity"]
    
    def analyze(self, text: str, file_name: str = "contract.txt", domain: str = "auto", user_mode: str = "staff") -> AnalysisResult:
        contract_type = self._detect_type(text)
        issues = []
        
        # ã‚³ã‚¢ã‚¨ãƒ³ã‚¸ãƒ³
        if CORE_AVAILABLE:
            for clause in self._split_clauses(text):
                result = quick_analyze(clause, domain=None if domain == "auto" else domain)
                if result["verdict"] in ["NG_CRITICAL", "NG", "REVIEW_HIGH"]:
                    self.issue_counter += 1
                    issues.append(Issue(issue_id=f"V166-{self.issue_counter:04d}", clause_text=clause[:200], issue_type=result["verdict"],
                        risk_level=self._to_risk(result["verdict"]), description=result["risk_summary"],
                        legal_basis=", ".join(result.get("legal_basis", [])[:3]), fix_suggestion=result["rewrite_suggestions"][0] if result["rewrite_suggestions"] else "å°‚é–€å®¶ã«ç›¸è«‡", category="v162ãƒ‘ã‚¿ãƒ¼ãƒ³"))
        
        # å±é™ºãƒ‘ã‚¿ãƒ¼ãƒ³
        seen = {i.clause_text[:50] for i in issues}
        for pid, pinfo in DANGER_PATTERNS.items():
            for pattern in pinfo["patterns"]:
                for match in re.finditer(pattern, text, re.I):
                    start, end = max(0, match.start() - 50), min(len(text), match.end() + 50)
                    context = text[start:end]
                    if context[:50] in seen:
                        continue
                    self.issue_counter += 1
                    issues.append(Issue(issue_id=f"LP-{self.issue_counter:04d}", clause_text=context, issue_type=pid, risk_level=pinfo["risk"],
                        description=pinfo["description"], legal_basis=pinfo["legal_basis"], fix_suggestion=pinfo["fix"], category=pinfo["category"]))
                    seen.add(context[:50])
        
        # Truth Engine
        truth_result = TruthEngine.analyze(text)
        
        # SMTæ¤œè¨¼
        smt_result = SMTVerifier.analyze(text)
        
        # PCRç”Ÿæˆ
        pcr_suggestions = PCREngine.generate(text, smt_result)
        
        # SMTæ¤œè¨¼ã‹ã‚‰Issueè¿½åŠ 
        for contradiction in smt_result.get("contradictions", []):
            self.issue_counter += 1
            issues.append(Issue(
                issue_id=f"SMT-{self.issue_counter:04d}",
                clause_text=contradiction.get("description", "")[:200],
                issue_type=contradiction.get("type", "CONTRADICTION"),
                risk_level=RiskLevel.CRITICAL if contradiction.get("severity") == "CRITICAL" else RiskLevel.HIGH,
                description=contradiction.get("description", "SMTæ¤œè¨¼ã§çŸ›ç›¾ã‚’æ¤œå‡º"),
                legal_basis=contradiction.get("axiom", ""),
                fix_suggestion="æ¡é …ã®æ•´åˆæ€§ã‚’ç¢ºèªã—ã€çŸ›ç›¾ã‚’è§£æ¶ˆã—ã¦ãã ã•ã„",
                category="SMTå½¢å¼æ¤œè¨¼",
                proof_id=smt_result.get("proof_id", ""),
            ))
        
        risk_score = min(100, sum({RiskLevel.CRITICAL: 30, RiskLevel.HIGH: 20, RiskLevel.MEDIUM: 10, RiskLevel.LOW: 5}.get(i.risk_level, 10) for i in issues))
        margin = max(5, 15 - len(issues))
        
        return AnalysisResult(issues=issues, risk_score=risk_score, confidence_interval=(max(0, risk_score - margin), min(100, risk_score + margin)),
            contract_type=contract_type, truth_result=truth_result, smt_result=smt_result, pcr_suggestions=pcr_suggestions, file_name=file_name)
    
    def _to_risk(self, verdict: str) -> RiskLevel:
        return {"NG_CRITICAL": RiskLevel.CRITICAL, "NG": RiskLevel.HIGH, "REVIEW_HIGH": RiskLevel.HIGH, "REVIEW_MED": RiskLevel.MEDIUM}.get(verdict, RiskLevel.MEDIUM)
    
    def _detect_type(self, text: str) -> ContractType:
        kw = {ContractType.NDA: ["ç§˜å¯†ä¿æŒ", "NDA"], ContractType.OUTSOURCING: ["æ¥­å‹™å§”è¨—", "è«‹è² "], ContractType.TOS: ["åˆ©ç”¨è¦ç´„", "ç´„æ¬¾"]}
        for ct, keywords in kw.items():
            if any(k in text for k in keywords):
                return ct
        return ContractType.GENERAL
    
    def _split_clauses(self, text: str) -> List[str]:
        clauses = re.findall(r"ç¬¬\s*\d+\s*æ¡[^ç¬¬]*", text, re.DOTALL)
        return clauses[:100] if clauses else [p.strip() for p in text.split("\n\n") if len(p.strip()) > 20][:100]


# =============================================================================
# ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
# =============================================================================
def extract_text(uploaded_file) -> str:
    ext = uploaded_file.name.split(".")[-1].lower()
    if ext == "txt":
        return uploaded_file.read().decode("utf-8", errors="ignore")
    elif ext == "pdf":
        try:
            import PyPDF2
            return "".join([p.extract_text() or "" for p in PyPDF2.PdfReader(io.BytesIO(uploaded_file.read())).pages])
        except:
            return "[PDFèª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼]"
    elif ext in ["doc", "docx"]:
        try:
            from docx import Document
            return "\n".join([p.text for p in Document(io.BytesIO(uploaded_file.read())).paragraphs])
        except:
            return "[Wordèª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼]"
    return uploaded_file.read().decode("utf-8", errors="ignore")


# =============================================================================
# UI
# =============================================================================
def render_badge(risk: RiskLevel) -> str:
    return {"CRITICAL": "ğŸ”´", "HIGH": "ğŸŸ ", "MEDIUM": "ğŸŸ¡", "LOW": "ğŸŸ¢", "SAFE": "âšª"}.get(risk.value, "âšª") + f" {risk.value}"

def render_smt_result(result: Dict):
    if not result:
        return
    st.markdown("### ğŸ” SMTå½¢å¼æ¤œè¨¼çµæœ")
    
    smt_status = result.get("smt_result", "UNKNOWN")
    status_colors = {"SAT": "ğŸŸ¢", "UNSAT": "ğŸ”´", "UNKNOWN": "ğŸŸ¡"}
    
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("æ¤œè¨¼çµæœ", f"{status_colors.get(smt_status, 'âšª')} {smt_status}")
    c2.metric("Truth Score", f"{result.get('truth_score', 0):.0f}/100")
    c3.metric("éé©åˆåº¦", f"{result.get('nonconformity_score', 0):.2f}")
    c4.metric("å‘½é¡Œæ•°", result.get("propositions_count", 0))
    
    ci = result.get("confidence_interval", (0, 100))
    st.caption(f"95%ä¿¡é ¼åŒºé–“: [{ci[0]:.0f}, {ci[1]:.0f}]")
    
    if result.get("fol_formulas"):
        with st.expander("ğŸ“ æŠ½å‡ºã•ã‚ŒãŸè«–ç†å¼ (FOL)"):
            for fol in result["fol_formulas"]:
                st.code(fol, language="text")
    
    if result.get("contradictions"):
        st.markdown("#### âš ï¸ æ¤œå‡ºã•ã‚ŒãŸçŸ›ç›¾")
        for c in result["contradictions"]:
            severity_icon = {"CRITICAL": "ğŸ”´", "HIGH": "ğŸŸ ", "MEDIUM": "ğŸŸ¡"}.get(c.get("severity"), "âšª")
            st.error(f"{severity_icon} **{c.get('type', 'CONTRADICTION')}**: {c.get('description')}")
            if c.get("axiom"):
                st.caption(f"é•åå…¬ç†: {c['axiom']}")
    
    if result.get("proof_id"):
        st.success(f"ğŸ” è¨¼æ˜ID: **{result['proof_id']}**")

def render_pcr_result(pcr_list: List[Dict]):
    if not pcr_list:
        return
    st.markdown("### ğŸ“ è¨¼æ˜ä»˜ãä¿®æ­£æ¡ˆ (PCR)")
    
    for pcr in pcr_list:
        with st.expander(f"ğŸ”§ {pcr.get('issue', 'ä¿®æ­£æ¡ˆ')} - {pcr.get('id', '')}", expanded=True):
            c1, c2 = st.columns(2)
            with c1:
                st.markdown("**âŒ å•é¡Œç®‡æ‰€**")
                st.error(pcr.get("original", ""))
            with c2:
                st.markdown("**âœ… ä¿®æ­£æ¡ˆ**")
                st.success(pcr.get("redline", ""))
            
            proof = pcr.get("proof", {})
            st.markdown("**ğŸ” è¨¼æ˜**")
            st.info(f"""
- **è¨¼æ˜ID**: {proof.get('proof_id', 'N/A')}
- **å‚ç…§å…¬ç†**: {proof.get('axiom', 'N/A')}
- **æ¤œè¨¼å†…å®¹**: {proof.get('verification', 'N/A')}
- **æ¤œè¨¼çµæœ**: {proof.get('result', 'N/A')}
- **SMTæ¤œè¨¼**: {'âœ… å®Œäº†' if proof.get('smt_verified') else 'â³ æœªæ¤œè¨¼'}
            """)


def main():
    with st.sidebar:
        st.header("âš™ï¸ VERITAS v166 è¨­å®š")
        st.subheader("ğŸ‘¤ ãƒ¢ãƒ¼ãƒ‰")
        st.session_state.user_mode = st.radio("è¡¨ç¤º", ["staff", "lawyer"], format_func=lambda x: "ğŸ‘¨â€ğŸ’¼ æ‹…å½“è€…" if x == "staff" else "âš–ï¸ å¼è­·å£«")
        st.markdown("---")
        st.subheader("ğŸ“Š ãƒªã‚¹ã‚¯è¨±å®¹åº¦")
        st.session_state.risk_tolerance = st.select_slider("æ„Ÿåº¦", list(RISK_PROFILES.keys()), value=st.session_state.risk_tolerance, format_func=lambda x: f"{RISK_PROFILES[x]['icon']} {RISK_PROFILES[x]['name']}")
        st.markdown("---")
        st.write(f"**v167 å®Œå…¨çµ±åˆç‰ˆ** | Core: {'âœ…' if CORE_AVAILABLE else 'âŒ'} | Z3: {'âœ…' if Z3_AVAILABLE else 'âŒ'} | å¼è­·å£«æ€è€ƒ: {'âœ…' if LAWYER_THINKING_AVAILABLE else 'âŒ'}")
        st.write(f"æ³•ä»¤å…¬ç†: {len(LEGAL_AXIOMS)}ä»¶")
        st.write(f"PCRãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {len(PCREngine.REDLINE_TEMPLATES)}ä»¶")

    st.title(f"ğŸ” VERITAS v167 {'âš–ï¸' if st.session_state.user_mode == 'lawyer' else 'ğŸ‘¨â€ğŸ’¼'}")
    st.caption("AIå¥‘ç´„æ›¸ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¨ãƒ³ã‚¸ãƒ³ã€å®Œå…¨çµ±åˆç‰ˆã€‘- Patent: 2025-159636")

    tabs = st.tabs(["ğŸ“„ åˆ†æ", "ğŸ§  å¼è­·å£«æ€è€ƒ", "ğŸ” SMTæ¤œè¨¼", "ğŸ“ PCRä¿®æ­£æ¡ˆ", "ğŸ“š æ³•ä»¤å…¬ç†", "ğŸ“ˆ å±¥æ­´"])

    with tabs[0]:
        st.header("ğŸ“„ å¥‘ç´„æ›¸åˆ†æ")
        uploaded = st.file_uploader("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["txt", "pdf", "doc", "docx"])
        text = st.text_area("ã¾ãŸã¯ç›´æ¥å…¥åŠ›", height=200)
        if uploaded:
            text = extract_text(uploaded)
            st.info(f"ğŸ“ {uploaded.name} ({len(text):,}æ–‡å­—)")
        if st.button("ğŸ” åˆ†æå®Ÿè¡Œ", type="primary", disabled=not text):
            with st.spinner("åˆ†æä¸­ï¼ˆSMTæ¤œè¨¼å«ã‚€ï¼‰..."):
                engine = VeritasEngine(st.session_state.risk_tolerance)
                result = engine.analyze(text, uploaded.name if uploaded else "input.txt", "auto", st.session_state.user_mode)
                st.session_state.current_analysis = result
                st.session_state.current_contract = text
                st.session_state.analysis_history.append({"timestamp": result.timestamp, "file_name": result.file_name, "risk_score": result.risk_score, "issue_count": len(result.issues)})
            st.success("âœ… åˆ†æå®Œäº†")
            
            c1, c2, c3, c4, c5 = st.columns(5)
            c1.metric("ãƒªã‚¹ã‚¯", f"{'ğŸ”´' if result.risk_score >= 70 else 'ğŸŸ ' if result.risk_score >= 40 else 'ğŸŸ¢'} {result.risk_score:.0f}")
            c2.metric("å•é¡Œ", len(result.issues))
            if result.smt_result:
                c3.metric("SMT", result.smt_result.get("smt_result", "N/A"))
                c4.metric("Truth", f"{result.smt_result.get('truth_score', 0):.0f}")
            c5.metric("PCR", len(result.pcr_suggestions))
            
            st.markdown("### ğŸš¨ æ¤œå‡ºå•é¡Œ")
            for issue in sorted(result.issues, key=lambda x: ["CRITICAL", "HIGH", "MEDIUM", "LOW", "SAFE"].index(x.risk_level.value)):
                with st.expander(f"{render_badge(issue.risk_level)} {issue.category} - {issue.issue_id}", expanded=issue.risk_level == RiskLevel.CRITICAL):
                    st.markdown(f"**èª¬æ˜:** {issue.description}\n\n**æ³•çš„æ ¹æ‹ :** {issue.legal_basis}\n\n**ä¿®æ­£ææ¡ˆ:** {issue.fix_suggestion}")
                    if issue.proof_id:
                        st.caption(f"ğŸ” è¨¼æ˜ID: {issue.proof_id}")
                    st.code(issue.clause_text)
            
            if result.smt_result:
                render_smt_result(result.smt_result)
            
            if result.pcr_suggestions:
                render_pcr_result(result.pcr_suggestions)

    with tabs[1]:
        st.header("ğŸ§  å¼è­·å£«æ€è€ƒåˆ†è§£åˆ†æ")
        st.markdown("""
        **v163æ–°æ©Ÿèƒ½**: å¼è­·å£«ã®æ€è€ƒæ§‹é€ ã‚’åˆ†è§£ã—ã€ä»¥ä¸‹ã®3è»¸ã§åˆ†æã—ã¾ã™ï¼š
        1. **æ›–æ˜§æ€§æ¤œå‡º** - å¸°çµæœªå®šç¾©ã€åˆ¤æ–­ä¸»ä½“ä¸æ˜ã€åŸºæº–æœªå®šç¾©
        2. **æ¡é …é–“æ•´åˆæ€§** - é‡è¤‡æ¡é …ã€åŠ¹æœã‚¿ã‚°ã®è¡çª
        3. **æœŸé–“æœªå®šç¾©** - è²¬ä»»æ¡é …ãƒ»è§£é™¤æ¨©ã®æœŸé–“ãƒã‚§ãƒƒã‚¯
        
        âœ… **å®Ÿè¨¼çµæœ**: å¼è­·å£«æŒ‡æ‘˜6/6é …ç›®(100%)è‡ªå‹•æ¤œå‡º
        """)
        
        lawyer_text = st.text_area("å¥‘ç´„æ›¸ãƒ†ã‚­ã‚¹ãƒˆ", st.session_state.get("current_contract", ""), height=200, key="lawyer_text")
        
        if st.button("ğŸ§  å¼è­·å£«æ€è€ƒåˆ†æ", type="primary") and lawyer_text and LAWYER_THINKING_AVAILABLE:
            with st.spinner("å¼è­·å£«æ€è€ƒãƒ‘ã‚¿ãƒ¼ãƒ³ã§åˆ†æä¸­..."):
                # æ¡é …ã‚’æŠ½å‡º
                clause_pattern = r'(ç¬¬\d+æ¡[ï¼ˆ(][^ï¼‰)]+[ï¼‰)])'
                clauses = []
                lines = lawyer_text.split('\n')
                current_num = None
                current_text = []
                
                for line in lines:
                    match = re.match(clause_pattern, line)
                    if match:
                        if current_num:
                            clauses.append((current_num, '\n'.join(current_text)))
                        current_num = match.group(1)
                        current_text = [line]
                    elif current_num:
                        current_text.append(line)
                if current_num:
                    clauses.append((current_num, '\n'.join(current_text)))
                
                # æ›–æ˜§æ€§æ¤œå‡º
                st.subheader("ğŸ” æ›–æ˜§æ€§æ¤œå‡º")
                ambiguity_count = 0
                for clause_num, clause_text in clauses:
                    results = analyze_ambiguity(clause_text, clause_num)
                    for r in results:
                        ambiguity_count += 1
                        with st.expander(f"{'ğŸ”´' if r.severity == 'HIGH' else 'ğŸŸ '} {r.clause_number}: {r.ambiguity_type.value}"):
                            st.markdown(f"**èª¬æ˜**: {r.explanation}")
                            st.markdown(f"**æ¨å¥¨**: {r.recommendation}")
                            st.code(r.trigger_text)
                
                if ambiguity_count == 0:
                    st.success("æ›–æ˜§æ€§ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                else:
                    st.warning(f"{ambiguity_count}ä»¶ã®æ›–æ˜§æ€§ã‚’æ¤œå‡º")
                
                # æ¡é …é–“æ•´åˆæ€§
                st.subheader("ğŸ”— æ¡é …é–“æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯")
                coherence_result = analyze_contract_coherence(lawyer_text)
                if coherence_result["results"]:
                    for r in coherence_result["results"]:
                        severity_icon = "ğŸ”´" if r.similarity_score >= 0.7 else "ğŸŸ " if r.similarity_score >= 0.5 else "ğŸŸ¡"
                        with st.expander(f"{severity_icon} {r.clause_a} â†” {r.clause_b} (é¡ä¼¼åº¦: {r.similarity_score:.0%})"):
                            st.markdown(f"**é‡è¤‡ã‚¿ã‚¤ãƒ—**: {r.overlap_type}")
                            st.markdown(f"**å…±é€šåŠ¹æœ**: {', '.join(r.shared_effects)}")
                            st.markdown(f"**æ¨å¥¨**: {r.recommendation}")
                else:
                    st.success("æ¡é …é–“ã®é‡è¤‡ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                
                # æœŸé–“æœªå®šç¾©
                st.subheader("â° æœŸé–“æœªå®šç¾©æ¤œå‡º")
                time_result = analyze_contract_time_limits(clauses)
                no_limit_results = [r for r in time_result["results"] if not r.has_time_limit]
                if no_limit_results:
                    for r in no_limit_results:
                        severity_icon = "ğŸ”´" if r.risk_level == "HIGH" else "ğŸŸ "
                        with st.expander(f"{severity_icon} {r.clause_number}: {r.category.value}"):
                            st.markdown(f"**èª¬æ˜**: {r.explanation}")
                            st.markdown(f"**æ¨å¥¨**: {r.recommendation}")
                else:
                    st.success("æœŸé–“æœªå®šç¾©ã®æ¡é …ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        
        elif not LAWYER_THINKING_AVAILABLE:
            st.error("å¼è­·å£«æ€è€ƒãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

    with tabs[2]:
        st.header("ğŸ” SMTå½¢å¼æ¤œè¨¼")
        st.markdown("""
        **SMT (Satisfiability Modulo Theories) ã‚½ãƒ«ãƒãƒ¼ã«ã‚ˆã‚‹å½¢å¼æ¤œè¨¼ï¼š**
        1. **å‘½é¡ŒæŠ½å‡º**: å¥‘ç´„æ¡é …ã‹ã‚‰è«–ç†å‘½é¡Œã‚’æŠ½å‡º
        2. **FOLå¤‰æ›**: ä¸€éšè¿°èªè«–ç†å¼ã«å¤‰æ›
        3. **å……è¶³å¯èƒ½æ€§åˆ¤å®š**: SATï¼ˆçŸ›ç›¾ãªã—ï¼‰/ UNSATï¼ˆçŸ›ç›¾ã‚ã‚Šï¼‰
        4. **ä¸å……è¶³ã‚³ã‚¢æŠ½å‡º**: çŸ›ç›¾ã®åŸå› ã¨ãªã‚‹å‘½é¡Œã‚’ç‰¹å®š
        """)
        
        text = st.text_area("æ¤œè¨¼ãƒ†ã‚­ã‚¹ãƒˆ", st.session_state.get("current_contract", ""), height=200, key="smt_text")
        if st.button("ğŸ” SMTæ¤œè¨¼å®Ÿè¡Œ", type="primary") and text:
            with st.spinner("å½¢å¼æ¤œè¨¼ä¸­..."):
                result = SMTVerifier.analyze(text)
                st.session_state.smt_result = result
            render_smt_result(result)

    with tabs[3]:
        st.header("ğŸ“ è¨¼æ˜ä»˜ãä¿®æ­£æ¡ˆ (PCR)")
        st.markdown("""
        **Proof-Carrying Redlines**: å½¢å¼çš„è¨¼æ˜ä»˜ãã®ä¿®æ­£æ¡ˆã‚’ç”Ÿæˆ
        - æ³•ä»¤å…¬ç†ã¨ã®æ•´åˆæ€§ã‚’æ¤œè¨¼
        - ä¿®æ­£å¾Œã®æ¡é …ãŒæ³•çš„è¦ä»¶ã‚’æº€ãŸã™ã“ã¨ã‚’è¨¼æ˜
        """)
        
        text = st.text_area("å¥‘ç´„æ›¸ãƒ†ã‚­ã‚¹ãƒˆ", st.session_state.get("current_contract", ""), height=200, key="pcr_text")
        if st.button("ğŸ“ PCRç”Ÿæˆ", type="primary") and text:
            with st.spinner("ä¿®æ­£æ¡ˆç”Ÿæˆä¸­..."):
                smt_result = SMTVerifier.analyze(text)
                pcr_list = PCREngine.generate(text, smt_result)
                st.session_state.pcr_result = pcr_list
            if pcr_list:
                render_pcr_result(pcr_list)
            else:
                st.info("ä¿®æ­£ãŒå¿…è¦ãªæ¡é …ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

    with tabs[4]:
        st.header("ğŸ“š æ³•ä»¤å…¬ç†ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹")
        st.markdown(f"**{len(LEGAL_AXIOMS)}ä»¶ã®æ³•ä»¤å…¬ç†ã‚’åéŒ²**")
        
        for law_id, axiom in LEGAL_AXIOMS.items():
            with st.expander(f"âš–ï¸ {law_id}: {axiom['name']}"):
                st.markdown(f"**å…¬ç†ï¼ˆFOLï¼‰**: `{axiom['axiom']}`")
                st.markdown(f"**èª¬æ˜**: {axiom['description']}")

    with tabs[5]:
        st.header("ğŸ“ˆ åˆ†æå±¥æ­´")
        if not st.session_state.analysis_history:
            st.info("å±¥æ­´ãªã—")
        else:
            for h in reversed(st.session_state.analysis_history):
                c1, c2, c3 = st.columns([3, 2, 1])
                c1.write(h.get("file_name", "?"))
                c2.write(h.get("timestamp", "")[:19])
                c3.write(f"{'ğŸ”´' if h.get('risk_score', 0) >= 70 else 'ğŸŸ ' if h.get('risk_score', 0) >= 40 else 'ğŸŸ¢'} {h.get('risk_score', 0):.0f}")
            if st.button("ğŸ—‘ï¸ ã‚¯ãƒªã‚¢"):
                st.session_state.analysis_history = []
                st.rerun()

if __name__ == "__main__":
    main()
